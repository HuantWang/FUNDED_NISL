-----label-----
1
-----code-----
static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
			  struct kvm_memory_slot *memslot, unsigned long hva,
			  unsigned long fault_status)
{
	int ret;
	bool write_fault, writable, hugetlb = false, force_pte = false;
	unsigned long mmu_seq;
	gfn_t gfn = fault_ipa >> PAGE_SHIFT;
	struct kvm *kvm = vcpu->kvm;
	struct kvm_mmu_memory_cache *memcache = &vcpu->arch.mmu_page_cache;
	struct vm_area_struct *vma;
	kvm_pfn_t pfn;
	pgprot_t mem_type = PAGE_S2;
	bool fault_ipa_uncached;
	bool logging_active = memslot_is_logging(memslot);
	unsigned long flags = 0;

	write_fault = kvm_is_write_fault(vcpu);
	if (fault_status == FSC_PERM && !write_fault) {
		kvm_err("Unexpected L2 read permission error\n");
		return -EFAULT;
	}

	/* Let's check if we will get back a huge page backed by hugetlbfs */
	down_read(&current->mm->mmap_sem);
	vma = find_vma_intersection(current->mm, hva, hva + 1);
	if (unlikely(!vma)) {
		kvm_err("Failed to find VMA for hva 0x%lx\n", hva);
		up_read(&current->mm->mmap_sem);
		return -EFAULT;
	}

	if (is_vm_hugetlb_page(vma) && !logging_active) {
		hugetlb = true;
		gfn = (fault_ipa & PMD_MASK) >> PAGE_SHIFT;
	} else {
		/*
		 * Pages belonging to memslots that don't have the same
		 * alignment for userspace and IPA cannot be mapped using
		 * block descriptors even if the pages belong to a THP for
		 * the process, because the stage-2 block descriptor will
		 * cover more than a single THP and we loose atomicity for
		 * unmapping, updates, and splits of the THP or other pages
		 * in the stage-2 block range.
		 */
		if ((memslot->userspace_addr & ~PMD_MASK) !=
		    ((memslot->base_gfn << PAGE_SHIFT) & ~PMD_MASK))
			force_pte = true;
	}
	up_read(&current->mm->mmap_sem);

	/* We need minimum second+third level pages */
	ret = mmu_topup_memory_cache(memcache, KVM_MMU_CACHE_MIN_PAGES,
				     KVM_NR_MEM_OBJS);
	if (ret)
		return ret;

	mmu_seq = vcpu->kvm->mmu_notifier_seq;
	/*
	 * Ensure the read of mmu_notifier_seq happens before we call
	 * gfn_to_pfn_prot (which calls get_user_pages), so that we don't risk
	 * the page we just got a reference to gets unmapped before we have a
	 * chance to grab the mmu_lock, which ensure that if the page gets
	 * unmapped afterwards, the call to kvm_unmap_hva will take it away
	 * from us again properly. This smp_rmb() interacts with the smp_wmb()
	 * in kvm_mmu_notifier_invalidate_<page|range_end>.
	 */
	smp_rmb();

	pfn = gfn_to_pfn_prot(kvm, gfn, write_fault, &writable);
	if (is_error_pfn(pfn))
		return -EFAULT;

	if (kvm_is_device_pfn(pfn)) {
		mem_type = PAGE_S2_DEVICE;
		flags |= KVM_S2PTE_FLAG_IS_IOMAP;
	} else if (logging_active) {
		/*
		 * Faults on pages in a memslot with logging enabled
		 * should not be mapped with huge pages (it introduces churn
		 * and performance degradation), so force a pte mapping.
		 */
		force_pte = true;
		flags |= KVM_S2_FLAG_LOGGING_ACTIVE;

		/*
		 * Only actually map the page as writable if this was a write
		 * fault.
		 */
		if (!write_fault)
			writable = false;
	}

	spin_lock(&kvm->mmu_lock);
	if (mmu_notifier_retry(kvm, mmu_seq))
		goto out_unlock;

	if (!hugetlb && !force_pte)
		hugetlb = transparent_hugepage_adjust(&pfn, &fault_ipa);

	fault_ipa_uncached = memslot->flags & KVM_MEMSLOT_INCOHERENT;

	if (hugetlb) {
		pmd_t new_pmd = pfn_pmd(pfn, mem_type);
		new_pmd = pmd_mkhuge(new_pmd);
		if (writable) {
			new_pmd = kvm_s2pmd_mkwrite(new_pmd);
			kvm_set_pfn_dirty(pfn);
		}
		coherent_cache_guest_page(vcpu, pfn, PMD_SIZE, fault_ipa_uncached);
		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd);
	} else {
		pte_t new_pte = pfn_pte(pfn, mem_type);

		if (writable) {
			new_pte = kvm_s2pte_mkwrite(new_pte);
			kvm_set_pfn_dirty(pfn);
			mark_page_dirty(kvm, gfn);
		}
		coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE, fault_ipa_uncached);
		ret = stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags);
	}

out_unlock:
	spin_unlock(&kvm->mmu_lock);
	kvm_set_pfn_accessed(pfn);
	kvm_release_pfn_clean(pfn);
	return ret;
}
-----children-----
1,2
1,3
1,4
3,4
3,5
3,6
3,7
3,8
3,9
5,6
5,7
6,7
8,9
8,10
11,12
11,13
12,13
14,15
16,17
16,18
17,18
19,20
19,21
22,23
22,24
24,25
26,27
26,28
28,29
30,31
30,32
30,33
30,34
30,35
30,36
30,37
30,38
30,39
30,40
30,41
30,42
30,43
30,44
30,45
30,46
30,47
30,48
30,49
30,50
30,51
30,52
30,53
30,54
30,55
30,56
30,57
30,58
30,59
30,60
30,61
30,62
30,63
30,64
30,65
31,32
32,33
32,34
34,35
36,37
37,38
37,39
37,40
37,41
37,42
39,40
41,42
43,44
43,45
45,46
47,48
47,49
49,50
51,52
52,53
52,54
54,55
56,57
57,58
57,59
58,59
60,61
60,62
62,63
63,64
63,65
64,65
66,67
68,69
69,70
69,71
70,71
72,73
72,74
72,75
75,76
76,77
76,78
77,78
80,81
81,82
81,83
82,83
84,85
84,86
84,87
87,88
88,89
89,90
89,91
90,91
90,92
91,92
95,96
96,97
96,98
97,98
99,100
99,101
102,103
103,104
103,105
104,105
106,107
108,109
109,110
109,111
110,111
112,113
112,114
114,115
115,116
117,118
118,119
118,120
120,121
122,123
123,124
123,125
125,126
125,127
127,128
128,129
128,130
129,130
131,132
133,134
134,135
134,136
136,137
136,138
138,139
140,141
141,142
141,143
142,143
144,145
144,146
145,146
147,148
149,150
149,151
150,151
150,152
151,152
151,153
152,153
154,155
156,157
157,158
159,160
159,161
160,161
161,162
161,163
162,163
165,166
166,167
167,168
169,170
170,171
170,172
171,172
173,174
174,175
174,176
175,176
175,177
176,177
180,181
181,182
181,183
182,183
184,185
184,186
184,187
184,188
185,186
187,188
187,189
188,189
191,192
193,194
193,195
194,195
197,198
197,199
198,199
198,200
199,200
201,202
202,203
204,205
204,206
204,207
205,206
206,207
206,208
206,209
207,208
210,211
212,213
213,214
213,215
214,215
216,217
217,218
217,219
218,219
218,220
219,220
223,224
224,225
225,226
227,228
227,229
227,230
228,229
228,230
229,230
229,231
230,231
232,233
234,235
235,236
237,238
237,239
238,239
239,240
239,241
240,241
243,244
244,245
244,246
245,246
247,248
247,249
248,249
249,250
249,251
250,251
252,253
254,255
256,257
257,258
257,259
258,259
258,260
259,260
260,261
260,262
261,262
261,263
262,263
265,266
266,267
268,269
269,270
269,271
270,271
271,272
271,273
272,273
272,274
273,274
276,277
278,279
279,280
281,282
282,283
282,284
283,284
286,287
287,288
287,289
288,289
290,291
291,292
291,293
292,293
292,294
293,294
297,298
298,299
298,300
299,300
301,302
301,303
301,304
301,305
302,303
304,305
306,307
308,309
310,311
310,312
311,312
313,314
314,315
316,317
317,318
317,319
318,319
320,321
320,322
321,322
321,323
322,323
326,327
327,328
328,329
330,331
331,332
331,333
332,333
334,335
334,336
334,337
334,338
334,339
335,336
337,338
339,340
341,342
343,344
344,345
346,347
346,348
347,348
347,349
348,349
350,351
352,353
353,354
354,355
356,357
356,358
356,359
357,358
357,359
358,359
360,361
362,363
362,364
363,364
364,365
364,366
365,366
367,368
369,370
370,371
370,372
371,372
373,374
375,376
375,377
376,377
378,379
378,380
378,381
379,380
380,381
380,382
381,382
384,385
385,386
385,387
386,387
388,389
390,391
390,392
391,392
392,393
394,395
395,396
395,397
396,397
399,400
400,401
400,402
401,402
403,404
404,405
404,406
405,406
408,409
408,410
409,410
409,411
409,412
410,411
412,413
414,415
416,417
418,419
418,420
419,420
419,421
420,421
421,422
423,424
424,425
426,427
427,428
427,429
428,429
430,431
430,432
430,433
431,432
433,434
434,435
436,437
437,438
439,440
440,441
440,442
441,442
443,444
443,445
444,445
444,446
445,446
448,449
450,451
450,452
450,453
451,452
453,454
453,455
453,456
453,457
453,458
454,455
455,456
455,457
456,457
458,459
458,460
460,461
461,462
461,463
461,464
462,463
464,465
466,467
468,469
469,470
469,471
470,471
472,473
472,474
473,474
475,476
477,478
477,479
478,479
480,481
480,482
481,482
482,483
482,484
483,484
485,486
485,487
486,487
488,489
490,491
491,492
491,493
492,493
494,495
496,497
497,498
497,499
497,500
497,501
497,502
498,499
500,501
502,503
504,505
506,507
508,509
509,510
509,511
510,511
512,513
512,514
512,515
512,516
512,517
513,514
515,516
517,518
519,520
521,522
522,523
524,525
524,526
524,527
524,528
525,526
526,527
526,528
527,528
529,530
529,531
531,532
532,533
532,534
532,535
533,534
535,536
537,538
539,540
539,541
540,541
542,543
542,544
542,545
543,544
544,545
544,546
545,546
547,548
547,549
548,549
550,551
552,553
553,554
553,555
554,555
556,557
558,559
559,560
559,561
559,562
560,561
562,563
564,565
566,567
567,568
567,569
567,570
567,571
567,572
568,569
570,571
572,573
574,575
576,577
578,579
579,580
579,581
580,581
582,583
582,584
582,585
582,586
582,587
582,588
583,584
585,586
587,588
589,590
591,592
592,593
594,595
596,597
596,598
598,599
599,600
599,601
600,601
602,603
603,604
603,605
604,605
607,608
608,609
608,610
609,610
611,612
613,614
614,615
614,616
615,616
617,618
619,620
620,621
-----nextToken-----
2,4,7,9,10,13,15,18,20,21,23,25,27,29,33,35,38,40,42,44,46,48,50,53,55,59,61,65,67,71,73,74,78,79,83,85,86,92,93,94,98,100,101,105,107,111,113,116,119,121,124,126,130,132,135,137,139,143,146,148,153,155,158,163,164,168,172,177,178,179,183,186,189,190,192,195,196,200,203,208,209,211,215,220,221,222,226,231,233,236,241,242,246,251,253,255,263,264,267,274,275,277,280,284,285,289,294,295,296,300,303,305,307,309,312,315,319,323,324,325,329,333,336,338,340,342,345,349,351,355,359,361,366,368,372,374,377,382,383,387,389,393,397,398,402,406,407,411,413,415,417,422,425,429,432,435,438,442,446,447,449,452,457,459,463,465,467,471,474,476,479,484,487,489,493,495,499,501,503,505,507,511,514,516,518,520,523,528,530,534,536,538,541,546,549,551,555,557,561,563,565,569,571,573,575,577,581,584,586,588,590,593,595,597,601,605,606,610,612,616,618,621
-----computeFrom-----
63,64
63,65
141,142
141,143
150,151
150,152
151,152
151,153
181,182
181,183
193,194
193,195
228,229
228,230
239,240
239,241
244,245
244,246
247,248
247,249
249,250
249,251
258,259
258,260
260,261
260,262
269,270
269,271
271,272
271,273
282,283
282,284
298,299
298,300
317,318
317,319
331,332
331,333
364,365
364,366
370,371
370,372
380,381
380,382
385,386
385,387
395,396
395,397
419,420
419,421
427,428
427,429
440,441
440,442
443,444
443,445
469,470
469,471
482,483
482,484
509,510
509,511
544,545
544,546
579,580
579,581
-----guardedBy-----
422,429
-----guardedByNegation-----
-----lastLexicalUse-----
-----jump-----
-----attribute-----
FunctionDefinition;SimpleDeclSpecifier;FunctionDeclarator;Name;ParameterDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;ParameterDeclaration;NamedTypeSpecifier;Name;Declarator;Name;ParameterDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;ParameterDeclaration;SimpleDeclSpecifier;Declarator;Name;ParameterDeclaration;SimpleDeclSpecifier;Declarator;Name;CompoundStatement;DeclarationStatement;SimpleDeclaration;SimpleDeclSpecifier;Declarator;Name;DeclarationStatement;SimpleDeclaration;SimpleDeclSpecifier;Declarator;Name;Declarator;Name;Declarator;Name;EqualsInitializer;LiteralExpression;Declarator;Name;EqualsInitializer;LiteralExpression;DeclarationStatement;SimpleDeclaration;SimpleDeclSpecifier;Declarator;Name;DeclarationStatement;SimpleDeclaration;NamedTypeSpecifier;Name;Declarator;Name;EqualsInitializer;BinaryExpression;IdExpression;Name;IdExpression;Name;DeclarationStatement;SimpleDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;EqualsInitializer;FieldReference;IdExpression;Name;Name;DeclarationStatement;SimpleDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;EqualsInitializer;UnaryExpression;FieldReference;FieldReference;IdExpression;Name;Name;Name;DeclarationStatement;SimpleDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;DeclarationStatement;SimpleDeclaration;NamedTypeSpecifier;Name;Declarator;Name;DeclarationStatement;SimpleDeclaration;NamedTypeSpecifier;Name;Declarator;Name;EqualsInitializer;IdExpression;Name;DeclarationStatement;SimpleDeclaration;SimpleDeclSpecifier;Declarator;Name;DeclarationStatement;SimpleDeclaration;SimpleDeclSpecifier;Declarator;Name;EqualsInitializer;FunctionCallExpression;IdExpression;Name;IdExpression;Name;DeclarationStatement;SimpleDeclaration;SimpleDeclSpecifier;Declarator;Name;EqualsInitializer;LiteralExpression;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IfStatement;BinaryExpression;BinaryExpression;IdExpression;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;CompoundStatement;ExpressionStatement;FunctionCallExpression;IdExpression;Name;LiteralExpression;ReturnStatement;UnaryExpression;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;UnaryExpression;FieldReference;FieldReference;IdExpression;Name;Name;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;FieldReference;IdExpression;Name;Name;IdExpression;Name;BinaryExpression;IdExpression;Name;LiteralExpression;IfStatement;FunctionCallExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;CompoundStatement;ExpressionStatement;FunctionCallExpression;IdExpression;Name;LiteralExpression;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;UnaryExpression;FieldReference;FieldReference;IdExpression;Name;Name;Name;ReturnStatement;UnaryExpression;IdExpression;Name;IfStatement;BinaryExpression;FunctionCallExpression;IdExpression;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;CompoundStatement;ExpressionStatement;BinaryExpression;IdExpression;Name;LiteralExpression;ExpressionStatement;BinaryExpression;IdExpression;Name;BinaryExpression;UnaryExpression;BinaryExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;CompoundStatement;IfStatement;BinaryExpression;UnaryExpression;BinaryExpression;FieldReference;IdExpression;Name;Name;UnaryExpression;IdExpression;Name;UnaryExpression;BinaryExpression;UnaryExpression;BinaryExpression;FieldReference;IdExpression;Name;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;LiteralExpression;ExpressionStatement;FunctionCallExpression;IdExpression;Name;UnaryExpression;FieldReference;FieldReference;IdExpression;Name;Name;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;IfStatement;IdExpression;Name;ReturnStatement;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FieldReference;FieldReference;IdExpression;Name;Name;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;IfStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ReturnStatement;UnaryExpression;IdExpression;Name;IfStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;CompoundStatement;ExpressionStatement;BinaryExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;IdExpression;Name;IfStatement;IdExpression;Name;CompoundStatement;ExpressionStatement;BinaryExpression;IdExpression;Name;LiteralExpression;ExpressionStatement;BinaryExpression;IdExpression;Name;IdExpression;Name;IfStatement;UnaryExpression;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;LiteralExpression;ExpressionStatement;FunctionCallExpression;IdExpression;Name;UnaryExpression;FieldReference;IdExpression;Name;Name;IfStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;GotoStatement;Name;IfStatement;BinaryExpression;UnaryExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;BinaryExpression;FieldReference;IdExpression;Name;Name;IdExpression;Name;IfStatement;IdExpression;Name;CompoundStatement;DeclarationStatement;SimpleDeclaration;NamedTypeSpecifier;Name;Declarator;Name;EqualsInitializer;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IfStatement;IdExpression;Name;CompoundStatement;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;CompoundStatement;DeclarationStatement;SimpleDeclaration;NamedTypeSpecifier;Name;Declarator;Name;EqualsInitializer;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;IfStatement;IdExpression;Name;CompoundStatement;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;IdExpression;Name;LabelStatement;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;UnaryExpression;FieldReference;IdExpression;Name;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ReturnStatement;IdExpression;Name;
-----ast_node-----
static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,			  struct kvm_memory_slot *memslot, unsigned long hva,			  unsigned long fault_status){	int ret;	bool write_fault, writable, hugetlb = false, force_pte = false;	unsigned long mmu_seq;	gfn_t gfn = fault_ipa >> PAGE_SHIFT;	struct kvm *kvm = vcpu->kvm;	struct kvm_mmu_memory_cache *memcache = &vcpu->arch.mmu_page_cache;	struct vm_area_struct *vma;	kvm_pfn_t pfn;	pgprot_t mem_type = PAGE_S2;	bool fault_ipa_uncached;	bool logging_active = memslot_is_logging(memslot);	unsigned long flags = 0;	write_fault = kvm_is_write_fault(vcpu);	if (fault_status == FSC_PERM && !write_fault) {		kvm_err("Unexpected L2 read permission error\n");		return -EFAULT;	}	/* Let's check if we will get back a huge page backed by hugetlbfs */	down_read(&current->mm->mmap_sem);	vma = find_vma_intersection(current->mm, hva, hva + 1);	if (unlikely(!vma)) {		kvm_err("Failed to find VMA for hva 0x%lx\n", hva);		up_read(&current->mm->mmap_sem);		return -EFAULT;	}	if (is_vm_hugetlb_page(vma) && !logging_active) {		hugetlb = true;		gfn = (fault_ipa & PMD_MASK) >> PAGE_SHIFT;	} else {		/*		 * Pages belonging to memslots that don't have the same		 * alignment for userspace and IPA cannot be mapped using		 * block descriptors even if the pages belong to a THP for		 * the process, because the stage-2 block descriptor will		 * cover more than a single THP and we loose atomicity for		 * unmapping, updates, and splits of the THP or other pages		 * in the stage-2 block range.		 */		if ((memslot->userspace_addr & ~PMD_MASK) !=		    ((memslot->base_gfn << PAGE_SHIFT) & ~PMD_MASK))			force_pte = true;	}	up_read(&current->mm->mmap_sem);	/* We need minimum second+third level pages */	ret = mmu_topup_memory_cache(memcache, KVM_MMU_CACHE_MIN_PAGES,				     KVM_NR_MEM_OBJS);	if (ret)		return ret;	mmu_seq = vcpu->kvm->mmu_notifier_seq;	/*	 * Ensure the read of mmu_notifier_seq happens before we call	 * gfn_to_pfn_prot (which calls get_user_pages), so that we don't risk	 * the page we just got a reference to gets unmapped before we have a	 * chance to grab the mmu_lock, which ensure that if the page gets	 * unmapped afterwards, the call to kvm_unmap_hva will take it away	 * from us again properly. This smp_rmb() interacts with the smp_wmb()	 * in kvm_mmu_notifier_invalidate_<page|range_end>.	 */	smp_rmb();	pfn = gfn_to_pfn_prot(kvm, gfn, write_fault, &writable);	if (is_error_pfn(pfn))		return -EFAULT;	if (kvm_is_device_pfn(pfn)) {		mem_type = PAGE_S2_DEVICE;		flags |= KVM_S2PTE_FLAG_IS_IOMAP;	} else if (logging_active) {		/*		 * Faults on pages in a memslot with logging enabled		 * should not be mapped with huge pages (it introduces churn		 * and performance degradation), so force a pte mapping.		 */		force_pte = true;		flags |= KVM_S2_FLAG_LOGGING_ACTIVE;		/*		 * Only actually map the page as writable if this was a write		 * fault.		 */		if (!write_fault)			writable = false;	}	spin_lock(&kvm->mmu_lock);	if (mmu_notifier_retry(kvm, mmu_seq))		goto out_unlock;	if (!hugetlb && !force_pte)		hugetlb = transparent_hugepage_adjust(&pfn, &fault_ipa);	fault_ipa_uncached = memslot->flags & KVM_MEMSLOT_INCOHERENT;	if (hugetlb) {		pmd_t new_pmd = pfn_pmd(pfn, mem_type);		new_pmd = pmd_mkhuge(new_pmd);		if (writable) {			new_pmd = kvm_s2pmd_mkwrite(new_pmd);			kvm_set_pfn_dirty(pfn);		}		coherent_cache_guest_page(vcpu, pfn, PMD_SIZE, fault_ipa_uncached);		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd);	} else {		pte_t new_pte = pfn_pte(pfn, mem_type);		if (writable) {			new_pte = kvm_s2pte_mkwrite(new_pte);			kvm_set_pfn_dirty(pfn);			mark_page_dirty(kvm, gfn);		}		coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE, fault_ipa_uncached);		ret = stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags);	}out_unlock:	spin_unlock(&kvm->mmu_lock);	kvm_set_pfn_accessed(pfn);	kvm_release_pfn_clean(pfn);	return ret;}
static int
user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,			  struct kvm_memory_slot *memslot, unsigned long hva,			  unsigned long fault_status)
user_mem_abort
struct kvm_vcpu *vcpu
struct kvm_vcpu
kvm_vcpu
*vcpu
*
vcpu
phys_addr_t fault_ipa
phys_addr_t
phys_addr_t
fault_ipa
fault_ipa
struct kvm_memory_slot *memslot
struct kvm_memory_slot
kvm_memory_slot
*memslot
*
memslot
unsigned long hva
unsigned long
hva
hva
unsigned long fault_status
unsigned long
fault_status
fault_status
{	int ret;	bool write_fault, writable, hugetlb = false, force_pte = false;	unsigned long mmu_seq;	gfn_t gfn = fault_ipa >> PAGE_SHIFT;	struct kvm *kvm = vcpu->kvm;	struct kvm_mmu_memory_cache *memcache = &vcpu->arch.mmu_page_cache;	struct vm_area_struct *vma;	kvm_pfn_t pfn;	pgprot_t mem_type = PAGE_S2;	bool fault_ipa_uncached;	bool logging_active = memslot_is_logging(memslot);	unsigned long flags = 0;	write_fault = kvm_is_write_fault(vcpu);	if (fault_status == FSC_PERM && !write_fault) {		kvm_err("Unexpected L2 read permission error\n");		return -EFAULT;	}	/* Let's check if we will get back a huge page backed by hugetlbfs */	down_read(&current->mm->mmap_sem);	vma = find_vma_intersection(current->mm, hva, hva + 1);	if (unlikely(!vma)) {		kvm_err("Failed to find VMA for hva 0x%lx\n", hva);		up_read(&current->mm->mmap_sem);		return -EFAULT;	}	if (is_vm_hugetlb_page(vma) && !logging_active) {		hugetlb = true;		gfn = (fault_ipa & PMD_MASK) >> PAGE_SHIFT;	} else {		/*		 * Pages belonging to memslots that don't have the same		 * alignment for userspace and IPA cannot be mapped using		 * block descriptors even if the pages belong to a THP for		 * the process, because the stage-2 block descriptor will		 * cover more than a single THP and we loose atomicity for		 * unmapping, updates, and splits of the THP or other pages		 * in the stage-2 block range.		 */		if ((memslot->userspace_addr & ~PMD_MASK) !=		    ((memslot->base_gfn << PAGE_SHIFT) & ~PMD_MASK))			force_pte = true;	}	up_read(&current->mm->mmap_sem);	/* We need minimum second+third level pages */	ret = mmu_topup_memory_cache(memcache, KVM_MMU_CACHE_MIN_PAGES,				     KVM_NR_MEM_OBJS);	if (ret)		return ret;	mmu_seq = vcpu->kvm->mmu_notifier_seq;	/*	 * Ensure the read of mmu_notifier_seq happens before we call	 * gfn_to_pfn_prot (which calls get_user_pages), so that we don't risk	 * the page we just got a reference to gets unmapped before we have a	 * chance to grab the mmu_lock, which ensure that if the page gets	 * unmapped afterwards, the call to kvm_unmap_hva will take it away	 * from us again properly. This smp_rmb() interacts with the smp_wmb()	 * in kvm_mmu_notifier_invalidate_<page|range_end>.	 */	smp_rmb();	pfn = gfn_to_pfn_prot(kvm, gfn, write_fault, &writable);	if (is_error_pfn(pfn))		return -EFAULT;	if (kvm_is_device_pfn(pfn)) {		mem_type = PAGE_S2_DEVICE;		flags |= KVM_S2PTE_FLAG_IS_IOMAP;	} else if (logging_active) {		/*		 * Faults on pages in a memslot with logging enabled		 * should not be mapped with huge pages (it introduces churn		 * and performance degradation), so force a pte mapping.		 */		force_pte = true;		flags |= KVM_S2_FLAG_LOGGING_ACTIVE;		/*		 * Only actually map the page as writable if this was a write		 * fault.		 */		if (!write_fault)			writable = false;	}	spin_lock(&kvm->mmu_lock);	if (mmu_notifier_retry(kvm, mmu_seq))		goto out_unlock;	if (!hugetlb && !force_pte)		hugetlb = transparent_hugepage_adjust(&pfn, &fault_ipa);	fault_ipa_uncached = memslot->flags & KVM_MEMSLOT_INCOHERENT;	if (hugetlb) {		pmd_t new_pmd = pfn_pmd(pfn, mem_type);		new_pmd = pmd_mkhuge(new_pmd);		if (writable) {			new_pmd = kvm_s2pmd_mkwrite(new_pmd);			kvm_set_pfn_dirty(pfn);		}		coherent_cache_guest_page(vcpu, pfn, PMD_SIZE, fault_ipa_uncached);		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd);	} else {		pte_t new_pte = pfn_pte(pfn, mem_type);		if (writable) {			new_pte = kvm_s2pte_mkwrite(new_pte);			kvm_set_pfn_dirty(pfn);			mark_page_dirty(kvm, gfn);		}		coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE, fault_ipa_uncached);		ret = stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags);	}out_unlock:	spin_unlock(&kvm->mmu_lock);	kvm_set_pfn_accessed(pfn);	kvm_release_pfn_clean(pfn);	return ret;}
int ret;
int ret;
int
ret
ret
bool write_fault, writable, hugetlb = false, force_pte = false;
bool write_fault, writable, hugetlb = false, force_pte = false;
bool
write_fault
write_fault
writable
writable
hugetlb = false
hugetlb
= false
false
force_pte = false
force_pte
= false
false
unsigned long mmu_seq;
unsigned long mmu_seq;
unsigned long
mmu_seq
mmu_seq
gfn_t gfn = fault_ipa >> PAGE_SHIFT;
gfn_t gfn = fault_ipa >> PAGE_SHIFT;
gfn_t
gfn_t
gfn = fault_ipa >> PAGE_SHIFT
gfn
= fault_ipa >> PAGE_SHIFT
fault_ipa >> PAGE_SHIFT
fault_ipa
fault_ipa
PAGE_SHIFT
PAGE_SHIFT
struct kvm *kvm = vcpu->kvm;
struct kvm *kvm = vcpu->kvm;
struct kvm
kvm
*kvm = vcpu->kvm
*
kvm
= vcpu->kvm
vcpu->kvm
vcpu
vcpu
kvm
struct kvm_mmu_memory_cache *memcache = &vcpu->arch.mmu_page_cache;
struct kvm_mmu_memory_cache *memcache = &vcpu->arch.mmu_page_cache;
struct kvm_mmu_memory_cache
kvm_mmu_memory_cache
*memcache = &vcpu->arch.mmu_page_cache
*
memcache
= &vcpu->arch.mmu_page_cache
&vcpu->arch.mmu_page_cache
vcpu->arch.mmu_page_cache
vcpu->arch
vcpu
vcpu
arch
mmu_page_cache
struct vm_area_struct *vma;
struct vm_area_struct *vma;
struct vm_area_struct
vm_area_struct
*vma
*
vma
kvm_pfn_t pfn;
kvm_pfn_t pfn;
kvm_pfn_t
kvm_pfn_t
pfn
pfn
pgprot_t mem_type = PAGE_S2;
pgprot_t mem_type = PAGE_S2;
pgprot_t
pgprot_t
mem_type = PAGE_S2
mem_type
= PAGE_S2
PAGE_S2
PAGE_S2
bool fault_ipa_uncached;
bool fault_ipa_uncached;
bool
fault_ipa_uncached
fault_ipa_uncached
bool logging_active = memslot_is_logging(memslot);
bool logging_active = memslot_is_logging(memslot);
bool
logging_active = memslot_is_logging(memslot)
logging_active
= memslot_is_logging(memslot)
memslot_is_logging(memslot)
memslot_is_logging
memslot_is_logging
memslot
memslot
unsigned long flags = 0;
unsigned long flags = 0;
unsigned long
flags = 0
flags
= 0
0
write_fault = kvm_is_write_fault(vcpu);
write_fault = kvm_is_write_fault(vcpu)
write_fault
write_fault
kvm_is_write_fault(vcpu)
kvm_is_write_fault
kvm_is_write_fault
vcpu
vcpu
if (fault_status == FSC_PERM && !write_fault) {		kvm_err("Unexpected L2 read permission error\n");		return -EFAULT;	}
fault_status == FSC_PERM && !write_fault
fault_status == FSC_PERM
fault_status
fault_status
FSC_PERM
FSC_PERM
!write_fault
write_fault
write_fault
{		kvm_err("Unexpected L2 read permission error\n");		return -EFAULT;	}
kvm_err("Unexpected L2 read permission error\n");
kvm_err("Unexpected L2 read permission error\n")
kvm_err
kvm_err
"Unexpected L2 read permission error\n"
return -EFAULT;
-EFAULT
EFAULT
EFAULT
down_read(&current->mm->mmap_sem);
down_read(&current->mm->mmap_sem)
down_read
down_read
&current->mm->mmap_sem
current->mm->mmap_sem
current->mm
current
current
mm
mmap_sem
vma = find_vma_intersection(current->mm, hva, hva + 1);
vma = find_vma_intersection(current->mm, hva, hva + 1)
vma
vma
find_vma_intersection(current->mm, hva, hva + 1)
find_vma_intersection
find_vma_intersection
current->mm
current
current
mm
hva
hva
hva + 1
hva
hva
1
if (unlikely(!vma)) {		kvm_err("Failed to find VMA for hva 0x%lx\n", hva);		up_read(&current->mm->mmap_sem);		return -EFAULT;	}
unlikely(!vma)
unlikely
unlikely
!vma
vma
vma
{		kvm_err("Failed to find VMA for hva 0x%lx\n", hva);		up_read(&current->mm->mmap_sem);		return -EFAULT;	}
kvm_err("Failed to find VMA for hva 0x%lx\n", hva);
kvm_err("Failed to find VMA for hva 0x%lx\n", hva)
kvm_err
kvm_err
"Failed to find VMA for hva 0x%lx\n"
hva
hva
up_read(&current->mm->mmap_sem);
up_read(&current->mm->mmap_sem)
up_read
up_read
&current->mm->mmap_sem
current->mm->mmap_sem
current->mm
current
current
mm
mmap_sem
return -EFAULT;
-EFAULT
EFAULT
EFAULT
if (is_vm_hugetlb_page(vma) && !logging_active) {		hugetlb = true;		gfn = (fault_ipa & PMD_MASK) >> PAGE_SHIFT;	} else {		/*		 * Pages belonging to memslots that don't have the same		 * alignment for userspace and IPA cannot be mapped using		 * block descriptors even if the pages belong to a THP for		 * the process, because the stage-2 block descriptor will		 * cover more than a single THP and we loose atomicity for		 * unmapping, updates, and splits of the THP or other pages		 * in the stage-2 block range.		 */		if ((memslot->userspace_addr & ~PMD_MASK) !=		    ((memslot->base_gfn << PAGE_SHIFT) & ~PMD_MASK))			force_pte = true;	}
is_vm_hugetlb_page(vma) && !logging_active
is_vm_hugetlb_page(vma)
is_vm_hugetlb_page
is_vm_hugetlb_page
vma
vma
!logging_active
logging_active
logging_active
{		hugetlb = true;		gfn = (fault_ipa & PMD_MASK) >> PAGE_SHIFT;	}
hugetlb = true;
hugetlb = true
hugetlb
hugetlb
true
gfn = (fault_ipa & PMD_MASK) >> PAGE_SHIFT;
gfn = (fault_ipa & PMD_MASK) >> PAGE_SHIFT
gfn
gfn
(fault_ipa & PMD_MASK) >> PAGE_SHIFT
(fault_ipa & PMD_MASK)
fault_ipa & PMD_MASK
fault_ipa
fault_ipa
PMD_MASK
PMD_MASK
PAGE_SHIFT
PAGE_SHIFT
{		/*		 * Pages belonging to memslots that don't have the same		 * alignment for userspace and IPA cannot be mapped using		 * block descriptors even if the pages belong to a THP for		 * the process, because the stage-2 block descriptor will		 * cover more than a single THP and we loose atomicity for		 * unmapping, updates, and splits of the THP or other pages		 * in the stage-2 block range.		 */		if ((memslot->userspace_addr & ~PMD_MASK) !=		    ((memslot->base_gfn << PAGE_SHIFT) & ~PMD_MASK))			force_pte = true;	}
if ((memslot->userspace_addr & ~PMD_MASK) !=		    ((memslot->base_gfn << PAGE_SHIFT) & ~PMD_MASK))			force_pte = true;
(memslot->userspace_addr & ~PMD_MASK) !=		    ((memslot->base_gfn << PAGE_SHIFT) & ~PMD_MASK)
(memslot->userspace_addr & ~PMD_MASK)
memslot->userspace_addr & ~PMD_MASK
memslot->userspace_addr
memslot
memslot
userspace_addr
~PMD_MASK
PMD_MASK
PMD_MASK
((memslot->base_gfn << PAGE_SHIFT) & ~PMD_MASK)
(memslot->base_gfn << PAGE_SHIFT) & ~PMD_MASK
(memslot->base_gfn << PAGE_SHIFT)
memslot->base_gfn << PAGE_SHIFT
memslot->base_gfn
memslot
memslot
base_gfn
PAGE_SHIFT
PAGE_SHIFT
~PMD_MASK
PMD_MASK
PMD_MASK
force_pte = true;
force_pte = true
force_pte
force_pte
true
up_read(&current->mm->mmap_sem);
up_read(&current->mm->mmap_sem)
up_read
up_read
&current->mm->mmap_sem
current->mm->mmap_sem
current->mm
current
current
mm
mmap_sem
ret = mmu_topup_memory_cache(memcache, KVM_MMU_CACHE_MIN_PAGES,				     KVM_NR_MEM_OBJS);
ret = mmu_topup_memory_cache(memcache, KVM_MMU_CACHE_MIN_PAGES,				     KVM_NR_MEM_OBJS)
ret
ret
mmu_topup_memory_cache(memcache, KVM_MMU_CACHE_MIN_PAGES,				     KVM_NR_MEM_OBJS)
mmu_topup_memory_cache
mmu_topup_memory_cache
memcache
memcache
KVM_MMU_CACHE_MIN_PAGES
KVM_MMU_CACHE_MIN_PAGES
KVM_NR_MEM_OBJS
KVM_NR_MEM_OBJS
if (ret)		return ret;
ret
ret
return ret;
ret
ret
mmu_seq = vcpu->kvm->mmu_notifier_seq;
mmu_seq = vcpu->kvm->mmu_notifier_seq
mmu_seq
mmu_seq
vcpu->kvm->mmu_notifier_seq
vcpu->kvm
vcpu
vcpu
kvm
mmu_notifier_seq
smp_rmb();
smp_rmb()
smp_rmb
smp_rmb
pfn = gfn_to_pfn_prot(kvm, gfn, write_fault, &writable);
pfn = gfn_to_pfn_prot(kvm, gfn, write_fault, &writable)
pfn
pfn
gfn_to_pfn_prot(kvm, gfn, write_fault, &writable)
gfn_to_pfn_prot
gfn_to_pfn_prot
kvm
kvm
gfn
gfn
write_fault
write_fault
&writable
writable
writable
if (is_error_pfn(pfn))		return -EFAULT;
is_error_pfn(pfn)
is_error_pfn
is_error_pfn
pfn
pfn
return -EFAULT;
-EFAULT
EFAULT
EFAULT
if (kvm_is_device_pfn(pfn)) {		mem_type = PAGE_S2_DEVICE;		flags |= KVM_S2PTE_FLAG_IS_IOMAP;	} else if (logging_active) {		/*		 * Faults on pages in a memslot with logging enabled		 * should not be mapped with huge pages (it introduces churn		 * and performance degradation), so force a pte mapping.		 */		force_pte = true;		flags |= KVM_S2_FLAG_LOGGING_ACTIVE;		/*		 * Only actually map the page as writable if this was a write		 * fault.		 */		if (!write_fault)			writable = false;	}
kvm_is_device_pfn(pfn)
kvm_is_device_pfn
kvm_is_device_pfn
pfn
pfn
{		mem_type = PAGE_S2_DEVICE;		flags |= KVM_S2PTE_FLAG_IS_IOMAP;	}
mem_type = PAGE_S2_DEVICE;
mem_type = PAGE_S2_DEVICE
mem_type
mem_type
PAGE_S2_DEVICE
PAGE_S2_DEVICE
flags |= KVM_S2PTE_FLAG_IS_IOMAP;
flags |= KVM_S2PTE_FLAG_IS_IOMAP
flags
flags
KVM_S2PTE_FLAG_IS_IOMAP
KVM_S2PTE_FLAG_IS_IOMAP
if (logging_active) {		/*		 * Faults on pages in a memslot with logging enabled		 * should not be mapped with huge pages (it introduces churn		 * and performance degradation), so force a pte mapping.		 */		force_pte = true;		flags |= KVM_S2_FLAG_LOGGING_ACTIVE;		/*		 * Only actually map the page as writable if this was a write		 * fault.		 */		if (!write_fault)			writable = false;	}
logging_active
logging_active
{		/*		 * Faults on pages in a memslot with logging enabled		 * should not be mapped with huge pages (it introduces churn		 * and performance degradation), so force a pte mapping.		 */		force_pte = true;		flags |= KVM_S2_FLAG_LOGGING_ACTIVE;		/*		 * Only actually map the page as writable if this was a write		 * fault.		 */		if (!write_fault)			writable = false;	}
force_pte = true;
force_pte = true
force_pte
force_pte
true
flags |= KVM_S2_FLAG_LOGGING_ACTIVE;
flags |= KVM_S2_FLAG_LOGGING_ACTIVE
flags
flags
KVM_S2_FLAG_LOGGING_ACTIVE
KVM_S2_FLAG_LOGGING_ACTIVE
if (!write_fault)			writable = false;
!write_fault
write_fault
write_fault
writable = false;
writable = false
writable
writable
false
spin_lock(&kvm->mmu_lock);
spin_lock(&kvm->mmu_lock)
spin_lock
spin_lock
&kvm->mmu_lock
kvm->mmu_lock
kvm
kvm
mmu_lock
if (mmu_notifier_retry(kvm, mmu_seq))		goto out_unlock;
mmu_notifier_retry(kvm, mmu_seq)
mmu_notifier_retry
mmu_notifier_retry
kvm
kvm
mmu_seq
mmu_seq
goto out_unlock;
out_unlock
if (!hugetlb && !force_pte)		hugetlb = transparent_hugepage_adjust(&pfn, &fault_ipa);
!hugetlb && !force_pte
!hugetlb
hugetlb
hugetlb
!force_pte
force_pte
force_pte
hugetlb = transparent_hugepage_adjust(&pfn, &fault_ipa);
hugetlb = transparent_hugepage_adjust(&pfn, &fault_ipa)
hugetlb
hugetlb
transparent_hugepage_adjust(&pfn, &fault_ipa)
transparent_hugepage_adjust
transparent_hugepage_adjust
&pfn
pfn
pfn
&fault_ipa
fault_ipa
fault_ipa
fault_ipa_uncached = memslot->flags & KVM_MEMSLOT_INCOHERENT;
fault_ipa_uncached = memslot->flags & KVM_MEMSLOT_INCOHERENT
fault_ipa_uncached
fault_ipa_uncached
memslot->flags & KVM_MEMSLOT_INCOHERENT
memslot->flags
memslot
memslot
flags
KVM_MEMSLOT_INCOHERENT
KVM_MEMSLOT_INCOHERENT
if (hugetlb) {		pmd_t new_pmd = pfn_pmd(pfn, mem_type);		new_pmd = pmd_mkhuge(new_pmd);		if (writable) {			new_pmd = kvm_s2pmd_mkwrite(new_pmd);			kvm_set_pfn_dirty(pfn);		}		coherent_cache_guest_page(vcpu, pfn, PMD_SIZE, fault_ipa_uncached);		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd);	} else {		pte_t new_pte = pfn_pte(pfn, mem_type);		if (writable) {			new_pte = kvm_s2pte_mkwrite(new_pte);			kvm_set_pfn_dirty(pfn);			mark_page_dirty(kvm, gfn);		}		coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE, fault_ipa_uncached);		ret = stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags);	}
hugetlb
hugetlb
{		pmd_t new_pmd = pfn_pmd(pfn, mem_type);		new_pmd = pmd_mkhuge(new_pmd);		if (writable) {			new_pmd = kvm_s2pmd_mkwrite(new_pmd);			kvm_set_pfn_dirty(pfn);		}		coherent_cache_guest_page(vcpu, pfn, PMD_SIZE, fault_ipa_uncached);		ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd);	}
pmd_t new_pmd = pfn_pmd(pfn, mem_type);
pmd_t new_pmd = pfn_pmd(pfn, mem_type);
pmd_t
pmd_t
new_pmd = pfn_pmd(pfn, mem_type)
new_pmd
= pfn_pmd(pfn, mem_type)
pfn_pmd(pfn, mem_type)
pfn_pmd
pfn_pmd
pfn
pfn
mem_type
mem_type
new_pmd = pmd_mkhuge(new_pmd);
new_pmd = pmd_mkhuge(new_pmd)
new_pmd
new_pmd
pmd_mkhuge(new_pmd)
pmd_mkhuge
pmd_mkhuge
new_pmd
new_pmd
if (writable) {			new_pmd = kvm_s2pmd_mkwrite(new_pmd);			kvm_set_pfn_dirty(pfn);		}
writable
writable
{			new_pmd = kvm_s2pmd_mkwrite(new_pmd);			kvm_set_pfn_dirty(pfn);		}
new_pmd = kvm_s2pmd_mkwrite(new_pmd);
new_pmd = kvm_s2pmd_mkwrite(new_pmd)
new_pmd
new_pmd
kvm_s2pmd_mkwrite(new_pmd)
kvm_s2pmd_mkwrite
kvm_s2pmd_mkwrite
new_pmd
new_pmd
kvm_set_pfn_dirty(pfn);
kvm_set_pfn_dirty(pfn)
kvm_set_pfn_dirty
kvm_set_pfn_dirty
pfn
pfn
coherent_cache_guest_page(vcpu, pfn, PMD_SIZE, fault_ipa_uncached);
coherent_cache_guest_page(vcpu, pfn, PMD_SIZE, fault_ipa_uncached)
coherent_cache_guest_page
coherent_cache_guest_page
vcpu
vcpu
pfn
pfn
PMD_SIZE
PMD_SIZE
fault_ipa_uncached
fault_ipa_uncached
ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd);
ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd)
ret
ret
stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd)
stage2_set_pmd_huge
stage2_set_pmd_huge
kvm
kvm
memcache
memcache
fault_ipa
fault_ipa
&new_pmd
new_pmd
new_pmd
{		pte_t new_pte = pfn_pte(pfn, mem_type);		if (writable) {			new_pte = kvm_s2pte_mkwrite(new_pte);			kvm_set_pfn_dirty(pfn);			mark_page_dirty(kvm, gfn);		}		coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE, fault_ipa_uncached);		ret = stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags);	}
pte_t new_pte = pfn_pte(pfn, mem_type);
pte_t new_pte = pfn_pte(pfn, mem_type);
pte_t
pte_t
new_pte = pfn_pte(pfn, mem_type)
new_pte
= pfn_pte(pfn, mem_type)
pfn_pte(pfn, mem_type)
pfn_pte
pfn_pte
pfn
pfn
mem_type
mem_type
if (writable) {			new_pte = kvm_s2pte_mkwrite(new_pte);			kvm_set_pfn_dirty(pfn);			mark_page_dirty(kvm, gfn);		}
writable
writable
{			new_pte = kvm_s2pte_mkwrite(new_pte);			kvm_set_pfn_dirty(pfn);			mark_page_dirty(kvm, gfn);		}
new_pte = kvm_s2pte_mkwrite(new_pte);
new_pte = kvm_s2pte_mkwrite(new_pte)
new_pte
new_pte
kvm_s2pte_mkwrite(new_pte)
kvm_s2pte_mkwrite
kvm_s2pte_mkwrite
new_pte
new_pte
kvm_set_pfn_dirty(pfn);
kvm_set_pfn_dirty(pfn)
kvm_set_pfn_dirty
kvm_set_pfn_dirty
pfn
pfn
mark_page_dirty(kvm, gfn);
mark_page_dirty(kvm, gfn)
mark_page_dirty
mark_page_dirty
kvm
kvm
gfn
gfn
coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE, fault_ipa_uncached);
coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE, fault_ipa_uncached)
coherent_cache_guest_page
coherent_cache_guest_page
vcpu
vcpu
pfn
pfn
PAGE_SIZE
PAGE_SIZE
fault_ipa_uncached
fault_ipa_uncached
ret = stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags);
ret = stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags)
ret
ret
stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags)
stage2_set_pte
stage2_set_pte
kvm
kvm
memcache
memcache
fault_ipa
fault_ipa
&new_pte
new_pte
new_pte
flags
flags
out_unlock:	spin_unlock(&kvm->mmu_lock);
out_unlock
spin_unlock(&kvm->mmu_lock);
spin_unlock(&kvm->mmu_lock)
spin_unlock
spin_unlock
&kvm->mmu_lock
kvm->mmu_lock
kvm
kvm
mmu_lock
kvm_set_pfn_accessed(pfn);
kvm_set_pfn_accessed(pfn)
kvm_set_pfn_accessed
kvm_set_pfn_accessed
pfn
pfn
kvm_release_pfn_clean(pfn);
kvm_release_pfn_clean(pfn)
kvm_release_pfn_clean
kvm_release_pfn_clean
pfn
pfn
return ret;
ret
ret
-----joern-----
(305,182,0)
(112,310,0)
(123,170,0)
(132,182,0)
(209,210,0)
(171,277,0)
(309,141,0)
(49,229,0)
(265,30,0)
(155,86,0)
(213,311,0)
(151,160,0)
(215,230,0)
(250,201,0)
(31,194,0)
(143,92,0)
(173,289,0)
(13,182,0)
(51,26,0)
(314,216,0)
(90,87,0)
(5,0,0)
(306,303,0)
(169,16,0)
(245,265,0)
(290,4,0)
(145,311,0)
(249,182,0)
(58,117,0)
(15,182,0)
(283,132,0)
(178,182,0)
(14,182,0)
(203,21,0)
(242,8,0)
(198,212,0)
(85,52,0)
(17,180,0)
(127,148,0)
(214,93,0)
(193,45,0)
(3,182,0)
(151,93,0)
(260,231,0)
(171,182,0)
(262,32,0)
(298,218,0)
(123,142,0)
(303,278,0)
(39,28,0)
(250,93,0)
(55,100,0)
(183,260,0)
(99,182,0)
(229,182,0)
(79,188,0)
(195,272,0)
(297,79,0)
(227,280,0)
(143,133,0)
(284,161,0)
(302,89,0)
(299,311,0)
(223,38,0)
(107,201,0)
(93,151,0)
(21,203,0)
(32,107,0)
(141,32,0)
(286,182,0)
(81,223,0)
(105,210,0)
(294,201,0)
(271,26,0)
(280,38,0)
(119,194,0)
(10,26,0)
(21,235,0)
(108,112,0)
(268,201,0)
(98,151,0)
(6,272,0)
(57,182,0)
(223,26,0)
(174,208,0)
(48,121,0)
(191,170,0)
(176,113,0)
(184,225,0)
(5,7,0)
(107,164,0)
(139,11,0)
(7,5,0)
(53,182,0)
(247,7,0)
(216,133,0)
(259,176,0)
(189,182,0)
(1,261,0)
(308,45,0)
(56,89,0)
(291,51,0)
(26,201,0)
(140,110,0)
(130,182,0)
(261,154,0)
(314,192,0)
(307,260,0)
(163,129,0)
(275,261,0)
(145,286,0)
(172,169,0)
(240,106,0)
(236,8,0)
(204,307,0)
(197,230,0)
(278,16,0)
(26,223,0)
(186,182,0)
(89,92,0)
(160,151,0)
(264,21,0)
(40,250,0)
(95,216,0)
(212,198,0)
(120,34,0)
(52,85,0)
(52,289,0)
(141,286,0)
(158,311,0)
(285,165,0)
(262,182,0)
(32,182,0)
(39,128,0)
(129,67,0)
(258,154,0)
(226,154,0)
(34,277,0)
(292,181,0)
(2,155,0)
(101,16,0)
(126,160,0)
(179,182,0)
(206,28,0)
(26,87,0)
(166,182,0)
(225,32,0)
(157,272,0)
(26,181,0)
(161,299,0)
(192,46,0)
(263,62,0)
(282,257,0)
(246,67,0)
(8,127,0)
(277,182,0)
(219,32,0)
(211,294,0)
(65,164,0)
(201,33,0)
(24,182,0)
(62,263,0)
(277,272,0)
(87,182,0)
(260,201,0)
(80,113,0)
(220,115,0)
(125,283,0)
(0,182,0)
(131,4,0)
(315,292,0)
(145,32,0)
(260,307,0)
(239,260,0)
(122,15,0)
(109,247,0)
(231,182,0)
(29,27,0)
(279,141,0)
(7,247,0)
(115,263,0)
(300,26,0)
(106,164,0)
(170,258,0)
(36,182,0)
(54,16,0)
(19,171,0)
(33,310,0)
(0,5,0)
(27,45,0)
(306,190,0)
(307,196,0)
(134,260,0)
(72,62,0)
(150,281,0)
(127,8,0)
(187,208,0)
(314,171,0)
(102,28,0)
(208,201,0)
(307,67,0)
(167,160,0)
(26,4,0)
(168,145,0)
(128,39,0)
(233,196,0)
(110,229,0)
(263,15,0)
(170,281,0)
(42,182,0)
(199,107,0)
(104,182,0)
(180,43,0)
(289,310,0)
(83,262,0)
(196,148,0)
(152,192,0)
(256,263,0)
(45,27,0)
(288,141,0)
(286,27,0)
(47,299,0)
(76,306,0)
(38,82,0)
(67,12,0)
(252,107,0)
(18,135,0)
(272,277,0)
(80,86,0)
(232,21,0)
(18,182,0)
(164,182,0)
(16,182,0)
(296,281,0)
(113,176,0)
(292,198,0)
(107,32,0)
(11,125,0)
(20,38,0)
(135,208,0)
(30,265,0)
(165,182,0)
(241,30,0)
(145,310,0)
(155,15,0)
(248,115,0)
(203,177,0)
(4,237,0)
(71,62,0)
(92,143,0)
(149,306,0)
(283,125,0)
(185,258,0)
(301,30,0)
(175,273,0)
(110,112,0)
(250,164,0)
(230,182,0)
(251,203,0)
(274,54,0)
(265,117,0)
(257,46,0)
(269,218,0)
(60,219,0)
(62,159,0)
(161,218,0)
(169,310,0)
(202,145,0)
(124,56,0)
(86,80,0)
(26,261,0)
(250,82,0)
(263,159,0)
(16,169,0)
(22,112,0)
(218,237,0)
(231,85,0)
(93,43,0)
(92,89,0)
(100,159,0)
(304,231,0)
(4,87,0)
(141,148,0)
(26,51,0)
(41,233,0)
(132,283,0)
(97,200,0)
(280,299,0)
(196,233,0)
(26,231,0)
(263,115,0)
(142,180,0)
(299,161,0)
(75,258,0)
(312,182,0)
(35,196,0)
(287,182,0)
(313,100,0)
(273,82,0)
(67,129,0)
(111,276,0)
(38,280,0)
(207,134,0)
(266,121,0)
(164,106,0)
(161,230,0)
(107,93,0)
(63,61,0)
(8,218,0)
(116,250,0)
(201,182,0)
(117,177,0)
(64,165,0)
(156,33,0)
(217,125,0)
(281,182,0)
(261,87,0)
(314,188,0)
(224,289,0)
(144,141,0)
(106,181,0)
(270,277,0)
(276,182,0)
(137,87,0)
(294,229,0)
(107,252,0)
(276,32,0)
(254,148,0)
(210,105,0)
(91,7,0)
(141,310,0)
(198,255,0)
(142,165,0)
(114,182,0)
(222,95,0)
(314,34,0)
(138,182,0)
(198,292,0)
(260,181,0)
(112,110,0)
(125,11,0)
(84,33,0)
(221,252,0)
(170,121,0)
(100,133,0)
(8,32,0)
(37,142,0)
(177,117,0)
(135,18,0)
(303,306,0)
(233,127,0)
(146,127,0)
(176,235,0)
(88,107,0)
(161,32,0)
(238,80,0)
(129,196,0)
(28,39,0)
(73,152,0)
(86,155,0)
(85,231,0)
(160,181,0)
(89,56,0)
(61,32,0)
(205,106,0)
(273,32,0)
(107,16,0)
(267,145,0)
(51,311,0)
(293,247,0)
(50,294,0)
(278,303,0)
(117,265,0)
(223,299,0)
(45,235,0)
(295,52,0)
(118,145,0)
(210,15,0)
(142,123,0)
(171,134,0)
(30,235,0)
(244,182,0)
(289,52,0)
(33,201,0)
(212,32,0)
(15,263,0)
(28,201,0)
(78,161,0)
(44,107,0)
(128,182,0)
(74,260,0)
(77,182,0)
(162,180,0)
(8,230,0)
(96,26,0)
(68,286,0)
(170,123,0)
(208,135,0)
(177,203,0)
(229,110,0)
(171,51,0)
(272,231,0)
(94,212,0)
(136,148,0)
(255,198,0)
(234,32,0)
(228,255,0)
(147,56,0)
(260,134,0)
(9,113,0)
(66,278,0)
(27,286,0)
(243,182,0)
(59,11,0)
(134,148,0)
(25,182,0)
(113,182,0)
(219,12,0)
(105,210,1)
(168,267,1)
(117,58,1)
(107,93,1)
(31,54,1)
(283,125,1)
(234,130,1)
(210,209,1)
(145,32,1)
(84,268,1)
(277,272,1)
(157,195,1)
(106,240,1)
(93,151,1)
(231,85,1)
(95,222,1)
(52,289,1)
(247,109,1)
(33,310,1)
(18,135,1)
(233,41,1)
(314,188,1)
(100,313,1)
(156,84,1)
(255,198,1)
(280,227,1)
(151,160,1)
(248,122,1)
(142,180,1)
(108,22,1)
(289,310,1)
(16,169,1)
(176,259,1)
(228,123,1)
(125,11,1)
(21,232,1)
(144,219,1)
(7,91,1)
(295,289,1)
(209,15,1)
(169,310,1)
(309,279,1)
(83,276,1)
(115,220,1)
(61,32,1)
(203,21,1)
(7,247,1)
(113,176,1)
(174,134,1)
(65,281,1)
(238,155,1)
(110,140,1)
(257,282,1)
(40,116,1)
(155,15,1)
(212,32,1)
(291,145,1)
(155,2,1)
(171,19,1)
(106,181,1)
(290,218,1)
(250,201,1)
(281,296,1)
(204,239,1)
(262,32,1)
(298,225,1)
(56,124,1)
(246,103,1)
(301,241,1)
(8,230,1)
(261,87,1)
(219,60,1)
(1,258,1)
(62,159,1)
(67,129,1)
(206,4,1)
(39,28,1)
(80,238,1)
(258,185,1)
(118,253,1)
(33,156,1)
(86,80,1)
(183,207,1)
(87,137,1)
(197,215,1)
(202,118,1)
(70,294,1)
(294,201,1)
(177,117,1)
(261,275,1)
(32,107,1)
(5,7,1)
(109,293,1)
(123,142,1)
(225,32,1)
(162,17,1)
(52,295,1)
(161,230,1)
(265,30,1)
(4,87,1)
(315,212,1)
(142,165,1)
(130,229,1)
(214,180,1)
(72,71,1)
(129,196,1)
(145,286,1)
(225,184,1)
(73,257,1)
(307,67,1)
(170,191,1)
(240,205,1)
(26,87,1)
(195,270,1)
(268,164,1)
(88,234,1)
(134,260,1)
(6,157,1)
(20,253,1)
(191,142,1)
(85,52,1)
(117,265,1)
(241,203,1)
(139,59,1)
(69,225,1)
(293,121,1)
(276,111,1)
(89,56,1)
(129,163,1)
(170,281,1)
(98,160,1)
(141,286,1)
(160,167,1)
(11,139,1)
(161,32,1)
(169,172,1)
(30,235,1)
(89,302,1)
(86,155,1)
(151,98,1)
(221,44,1)
(269,298,1)
(308,68,1)
(207,141,1)
(80,113,1)
(259,9,1)
(223,81,1)
(119,31,1)
(206,69,1)
(149,76,1)
(59,303,1)
(0,5,1)
(28,201,1)
(161,218,1)
(58,265,1)
(289,224,1)
(292,315,1)
(41,35,1)
(282,303,1)
(232,264,1)
(112,108,1)
(272,6,1)
(26,201,1)
(112,310,1)
(9,230,1)
(107,252,1)
(250,40,1)
(251,21,1)
(96,223,1)
(66,306,1)
(198,292,1)
(26,261,1)
(250,93,1)
(8,32,1)
(101,87,1)
(140,112,1)
(185,75,1)
(19,262,1)
(260,231,1)
(171,277,1)
(45,193,1)
(294,229,1)
(26,223,1)
(127,8,1)
(216,95,1)
(100,159,1)
(206,194,1)
(274,261,1)
(37,294,1)
(293,93,1)
(10,291,1)
(22,49,1)
(215,231,1)
(47,153,1)
(265,245,1)
(173,304,1)
(284,78,1)
(176,235,1)
(205,65,1)
(177,203,1)
(44,199,1)
(60,67,1)
(314,216,1)
(75,69,1)
(263,115,1)
(314,171,1)
(260,201,1)
(272,231,1)
(242,146,1)
(171,51,1)
(26,231,1)
(293,177,1)
(21,235,1)
(227,20,1)
(211,128,1)
(250,164,1)
(62,72,1)
(153,286,1)
(175,38,1)
(102,206,1)
(180,162,1)
(26,4,1)
(303,306,1)
(288,144,1)
(144,103,1)
(278,66,1)
(126,214,1)
(45,235,1)
(278,16,1)
(91,247,1)
(90,113,1)
(296,150,1)
(172,101,1)
(2,105,1)
(163,246,1)
(51,26,1)
(188,79,1)
(124,147,1)
(306,190,1)
(147,100,1)
(224,173,1)
(161,284,1)
(107,201,1)
(264,86,1)
(252,221,1)
(118,250,1)
(230,197,1)
(194,119,1)
(276,32,1)
(164,106,1)
(121,266,1)
(184,61,1)
(280,299,1)
(262,83,1)
(68,255,1)
(206,54,1)
(131,290,1)
(48,177,1)
(273,175,1)
(314,192,1)
(292,181,1)
(107,164,1)
(27,29,1)
(122,132,1)
(314,34,1)
(263,62,1)
(223,299,1)
(143,92,1)
(286,27,1)
(218,269,1)
(54,274,1)
(270,0,1)
(34,277,1)
(208,201,1)
(300,10,1)
(222,143,1)
(271,300,1)
(146,153,1)
(135,208,1)
(94,228,1)
(49,23,1)
(78,47,1)
(142,37,1)
(8,236,1)
(35,127,1)
(27,45,1)
(266,48,1)
(307,204,1)
(210,15,1)
(196,233,1)
(170,121,1)
(125,217,1)
(26,96,1)
(199,88,1)
(203,251,1)
(256,115,1)
(103,196,1)
(220,248,1)
(160,181,1)
(167,126,1)
(263,159,1)
(8,218,1)
(229,110,1)
(267,202,1)
(15,263,1)
(297,61,1)
(208,187,1)
(174,51,1)
(303,278,1)
(141,310,1)
(212,94,1)
(313,55,1)
(74,183,1)
(81,271,1)
(38,280,1)
(120,23,1)
(111,18,1)
(217,11,1)
(71,256,1)
(198,212,1)
(223,38,1)
(132,283,1)
(150,165,1)
(170,258,1)
(260,307,1)
(128,39,1)
(28,102,1)
(92,89,1)
(174,70,1)
(141,32,1)
(23,277,1)
(245,30,1)
(107,16,1)
(192,152,1)
(233,127,1)
(273,32,1)
(63,32,1)
(34,120,1)
(236,242,1)
(4,131,1)
(55,105,1)
(307,196,1)
(279,288,1)
(68,123,1)
(79,297,1)
(141,309,1)
(152,73,1)
(239,74,1)
(110,112,1)
(145,310,1)
(165,64,1)
(116,273,1)
(29,45,1)
(61,63,1)
(26,181,1)
(145,168,1)
(294,50,1)
(201,33,1)
(253,299,1)
(302,56,1)
(137,90,1)
(304,201,1)
(193,308,1)
(299,161,1)
(275,1,1)
(171,134,1)
(306,149,1)
(187,174,1)
(17,86,1)
(76,16,1)
(219,32,1)
(260,181,1)
(64,285,1)
(54,16,1)
(30,301,1)
(50,211,1)
(123,170,1)
(168,153,2)
(153,294,2)
(273,253,2)
(62,159,2)
(26,261,2)
(234,23,2)
(236,153,2)
(102,61,2)
(31,54,2)
(277,272,2)
(107,164,2)
(204,153,2)
(257,303,2)
(250,93,2)
(145,310,2)
(141,286,2)
(228,123,2)
(216,105,2)
(83,61,2)
(302,105,2)
(151,160,2)
(26,153,2)
(242,153,2)
(26,181,2)
(188,61,2)
(128,61,2)
(10,153,2)
(203,21,2)
(130,23,2)
(219,103,2)
(122,303,2)
(191,142,2)
(252,23,2)
(118,153,2)
(78,153,2)
(44,23,2)
(59,303,2)
(170,121,2)
(34,277,2)
(121,177,2)
(16,169,2)
(79,61,2)
(89,105,2)
(93,86,2)
(209,303,2)
(56,105,2)
(162,86,2)
(258,69,2)
(164,106,2)
(141,153,2)
(288,153,2)
(180,86,2)
(262,61,2)
(286,27,2)
(289,310,2)
(307,67,2)
(32,107,2)
(92,105,2)
(270,105,2)
(49,23,2)
(307,153,2)
(306,190,2)
(26,4,2)
(152,303,2)
(51,153,2)
(81,153,2)
(116,253,2)
(45,294,2)
(274,69,2)
(33,310,2)
(129,103,2)
(167,86,2)
(264,86,2)
(145,32,2)
(45,235,2)
(192,303,2)
(47,153,2)
(247,105,2)
(142,165,2)
(117,86,2)
(38,280,2)
(123,294,2)
(297,61,2)
(134,260,2)
(232,86,2)
(5,7,2)
(275,69,2)
(260,231,2)
(223,38,2)
(106,181,2)
(170,142,2)
(198,212,2)
(263,62,2)
(75,69,2)
(123,142,2)
(134,153,2)
(103,153,2)
(21,86,2)
(199,23,2)
(111,61,2)
(18,135,2)
(74,153,2)
(26,87,2)
(223,299,2)
(66,306,2)
(113,176,2)
(260,153,2)
(194,54,2)
(7,247,2)
(294,201,2)
(63,23,2)
(278,16,2)
(263,115,2)
(127,8,2)
(278,306,2)
(32,23,2)
(61,23,2)
(110,112,2)
(314,171,2)
(15,303,2)
(251,86,2)
(141,310,2)
(0,105,2)
(151,86,2)
(298,225,2)
(86,105,2)
(280,253,2)
(50,61,2)
(95,105,2)
(117,265,2)
(206,61,2)
(107,201,2)
(171,277,2)
(214,86,2)
(293,105,2)
(62,303,2)
(201,33,2)
(294,229,2)
(19,61,2)
(142,294,2)
(314,192,2)
(110,23,2)
(196,153,2)
(265,86,2)
(8,153,2)
(300,153,2)
(8,218,2)
(115,303,2)
(171,51,2)
(308,294,2)
(48,177,2)
(142,180,2)
(175,253,2)
(51,26,2)
(303,306,2)
(256,303,2)
(212,123,2)
(41,153,2)
(28,61,2)
(107,23,2)
(217,303,2)
(30,86,2)
(143,92,2)
(309,153,2)
(27,45,2)
(202,153,2)
(231,85,2)
(35,153,2)
(220,303,2)
(23,105,2)
(246,103,2)
(93,151,2)
(314,34,2)
(68,294,2)
(123,170,2)
(245,86,2)
(174,61,2)
(155,15,2)
(70,294,2)
(210,15,2)
(203,86,2)
(163,103,2)
(145,286,2)
(198,123,2)
(107,252,2)
(27,294,2)
(184,61,2)
(29,294,2)
(185,69,2)
(260,181,2)
(222,105,2)
(292,181,2)
(28,201,2)
(8,32,2)
(219,32,2)
(266,177,2)
(161,32,2)
(177,203,2)
(195,105,2)
(161,153,2)
(250,201,2)
(17,86,2)
(262,32,2)
(131,225,2)
(177,86,2)
(221,23,2)
(229,110,2)
(20,253,2)
(314,216,2)
(26,231,2)
(145,153,2)
(265,30,2)
(299,161,2)
(8,230,2)
(140,23,2)
(170,281,2)
(263,159,2)
(7,105,2)
(241,86,2)
(233,153,2)
(26,201,2)
(98,86,2)
(161,218,2)
(279,153,2)
(263,303,2)
(39,61,2)
(71,303,2)
(125,303,2)
(271,153,2)
(177,117,2)
(96,153,2)
(146,153,2)
(21,235,2)
(124,105,2)
(67,103,2)
(67,129,2)
(272,231,2)
(100,105,2)
(119,54,2)
(291,153,2)
(139,303,2)
(107,93,2)
(105,303,2)
(183,153,2)
(155,105,2)
(109,105,2)
(126,86,2)
(273,32,2)
(157,105,2)
(132,303,2)
(4,87,2)
(284,153,2)
(58,86,2)
(141,32,2)
(225,61,2)
(15,263,2)
(301,86,2)
(0,5,2)
(160,86,2)
(69,225,2)
(5,105,2)
(283,303,2)
(40,253,2)
(91,105,2)
(4,225,2)
(86,155,2)
(127,153,2)
(52,289,2)
(61,32,2)
(286,294,2)
(272,105,2)
(85,52,2)
(54,16,2)
(227,253,2)
(292,123,2)
(39,28,2)
(22,23,2)
(280,299,2)
(112,23,2)
(290,225,2)
(267,153,2)
(34,23,2)
(6,105,2)
(161,230,2)
(239,153,2)
(2,105,2)
(248,303,2)
(212,32,2)
(261,87,2)
(120,23,2)
(108,23,2)
(223,153,2)
(125,11,2)
(171,61,2)
(1,69,2)
(196,233,2)
(283,125,2)
(313,105,2)
(55,105,2)
(250,253,2)
(105,210,2)
(260,307,2)
(94,123,2)
(80,113,2)
(26,223,2)
(233,127,2)
(255,123,2)
(303,278,2)
(129,196,2)
(107,16,2)
(269,225,2)
(170,258,2)
(314,188,2)
(299,153,2)
(73,303,2)
(132,283,2)
(294,61,2)
(229,23,2)
(89,56,2)
(208,201,2)
(100,159,2)
(86,80,2)
(225,32,2)
(147,105,2)
(72,303,2)
(169,310,2)
(171,134,2)
(276,61,2)
(135,61,2)
(176,235,2)
(276,32,2)
(160,181,2)
(135,208,2)
(260,201,2)
(18,61,2)
(144,153,2)
(208,61,2)
(253,153,2)
(255,198,2)
(143,105,2)
(307,196,2)
(112,310,2)
(60,103,2)
(54,69,2)
(250,164,2)
(38,253,2)
(92,89,2)
(261,69,2)
(37,294,2)
(218,225,2)
(128,39,2)
(210,303,2)
(11,303,2)
(238,155,2)
(80,155,2)
(198,292,2)
(187,61,2)
(207,153,2)
(277,105,2)
(211,61,2)
(315,123,2)
(282,303,2)
(88,23,2)
(30,235,2)
(193,294,2)
-----------------------------------
(0,up_read(&current->mm->mmap_sem)
(1,flags)
(2,vma)
(3,ret)
(4,flags |= KVM_S2PTE_FLAG_IS_IOMAP)
(5,&current->mm->mmap_sem)
(6,KVM_NR_MEM_OBJS)
(7,current->mm->mmap_sem)
(8,pfn_pmd(pfn, mem_type)
(9,logging_active)
(10,kvm)
(11,current->mm)
(12,)
(13,writable)
(14,if (fault_status == FSC_PERM && !write_fault)
(15,vma = find_vma_intersection(current->mm, hva, hva + 1)
(16,write_fault = kvm_is_write_fault(vcpu)
(17,hugetlb)
(18,spin_unlock(&kvm->mmu_lock)
(19,ret)
(20,new_pte)
(21,memslot->userspace_addr)
(22,vcpu)
(23,ret)
(24,if (is_vm_hugetlb_page(vma)
(25,vma)
(26,stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags)
(27,memslot->flags & KVM_MEMSLOT_INCOHERENT)
(28,kvm->mmu_lock)
(29,KVM_MEMSLOT_INCOHERENT)
(30,memslot->base_gfn)
(31,writable)
(32,pfn = gfn_to_pfn_prot(kvm, gfn, write_fault, &writable)
(33,vcpu->kvm)
(34,return ret;)
(35,new_pmd)
(36,hugetlb)
(37,hugetlb)
(38,new_pte = kvm_s2pte_mkwrite(new_pte)
(39,&kvm->mmu_lock)
(40,gfn)
(41,new_pmd)
(42,memcache)
(43,)
(44,write_fault)
(45,memslot->flags)
(46,)
(47,new_pte)
(48,force_pte)
(49,mmu_seq)
(50,mmu_seq)
(51,ret = stage2_set_pte(kvm, memcache, fault_ipa, &new_pte, flags)
(52,vcpu->arch.mmu_page_cache)
(53,if (is_error_pfn(pfn)
(54,!write_fault)
(55,"Failed to find VMA for hva 0x%lx\\n")
(56,current->mm)
(57,pfn)
(58,~PMD_MASK)
(59,current)
(60,pfn)
(61,is_error_pfn(pfn)
(62,hva + 1)
(63,pfn)
(64,false)
(65,gfn)
(66,write_fault)
(67,new_pmd = kvm_s2pmd_mkwrite(new_pmd)
(68,fault_ipa_uncached)
(69,logging_active)
(70,goto out_unlock;)
(71,hva)
(72,1)
(73,EFAULT)
(74,memcache)
(75,force_pte)
(76,fault_status)
(77,out_unlock:)
(78,pfn)
(79,-EFAULT)
(80,!logging_active)
(81,new_pte)
(82,)
(83,pfn)
(84,vcpu)
(85,&vcpu->arch.mmu_page_cache)
(86,is_vm_hugetlb_page(vma)
(87,flags = 0)
(88,kvm)
(89,current->mm->mmap_sem)
(90,flags)
(91,mmap_sem)
(92,&current->mm->mmap_sem)
(93,gfn = (fault_ipa & PMD_MASK)
(94,pfn)
(95,-EFAULT)
(96,flags)
(97,if ((memslot->userspace_addr & ~PMD_MASK)
(98,PAGE_SHIFT)
(99,fault_ipa_uncached)
(100,kvm_err("Failed to find VMA for hva 0x%lx\\n", hva)
(101,write_fault)
(102,mmu_lock)
(103,writable)
(104,flags)
(105,unlikely(!vma)
(106,fault_ipa >> PAGE_SHIFT)
(107,gfn_to_pfn_prot(kvm, gfn, write_fault, &writable)
(108,kvm)
(109,mm)
(110,vcpu->kvm->mmu_notifier_seq)
(111,pfn)
(112,vcpu->kvm)
(113,logging_active = memslot_is_logging(memslot)
(114,write_fault)
(115,current->mm)
(116,kvm)
(117,(memslot->base_gfn << PAGE_SHIFT)
(118,vcpu)
(119,false)
(120,ret)
(121,force_pte = true)
(122,vma)
(123,!hugetlb && !force_pte)
(124,mm)
(125,current->mm->mmap_sem)
(126,fault_ipa)
(127,new_pmd = pfn_pmd(pfn, mem_type)
(128,spin_lock(&kvm->mmu_lock)
(129,kvm_s2pmd_mkwrite(new_pmd)
(130,smp_rmb()
(131,KVM_S2PTE_FLAG_IS_IOMAP)
(132,down_read(&current->mm->mmap_sem)
(133,)
(134,ret = stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd)
(135,&kvm->mmu_lock)
(136,if (writable)
(137,0)
(138,if (ret)
(139,mm)
(140,mmu_notifier_seq)
(141,coherent_cache_guest_page(vcpu, pfn, PMD_SIZE, fault_ipa_uncached)
(142,!hugetlb)
(143,up_read(&current->mm->mmap_sem)
(144,vcpu)
(145,coherent_cache_guest_page(vcpu, pfn, PAGE_SIZE, fault_ipa_uncached)
(146,new_pmd)
(147,current)
(148,)
(149,FSC_PERM)
(150,force_pte)
(151,(fault_ipa & PMD_MASK)
(152,-EFAULT)
(153,hugetlb)
(154,)
(155,is_vm_hugetlb_page(vma)
(156,kvm)
(157,KVM_MMU_CACHE_MIN_PAGES)
(158,new_pte)
(159,unsigned long hva)
(160,fault_ipa & PMD_MASK)
(161,pfn_pte(pfn, mem_type)
(162,true)
(163,new_pmd)
(164,gfn = fault_ipa >> PAGE_SHIFT)
(165,hugetlb = false)
(166,if (kvm_is_device_pfn(pfn)
(167,PMD_MASK)
(168,fault_ipa_uncached)
(169,kvm_is_write_fault(vcpu)
(170,!force_pte)
(171,return ret;)
(172,vcpu)
(173,vcpu)
(174,kvm)
(175,pfn)
(176,memslot_is_logging(memslot)
(177,(memslot->userspace_addr & ~PMD_MASK)
(178,force_pte)
(179,if (unlikely(!vma)
(180,hugetlb = true)
(181,phys_addr_t fault_ipa)
(182,)
(183,kvm)
(184,pfn)
(185,true)
(186,logging_active)
(187,mmu_lock)
(188,return -EFAULT;)
(189,if (!hugetlb && !force_pte)
(190,unsigned long fault_status)
(191,force_pte)
(192,return -EFAULT;)
(193,flags)
(194,writable = false)
(195,memcache)
(196,new_pmd = pmd_mkhuge(new_pmd)
(197,PAGE_S2)
(198,transparent_hugepage_adjust(&pfn, &fault_ipa)
(199,gfn)
(200,)
(201,*kvm = vcpu->kvm)
(202,pfn)
(203,memslot->userspace_addr & ~PMD_MASK)
(204,new_pmd)
(205,fault_ipa)
(206,kvm)
(207,ret)
(208,kvm->mmu_lock)
(209,vma)
(210,!vma)
(211,kvm)
(212,&pfn)
(213,if (writable)
(214,gfn)
(215,mem_type)
(216,return -EFAULT;)
(217,mmap_sem)
(218,mem_type = PAGE_S2_DEVICE)
(219,kvm_set_pfn_dirty(pfn)
(220,mm)
(221,writable)
(222,EFAULT)
(223,&new_pte)
(224,arch)
(225,kvm_is_device_pfn(pfn)
(226,if (!write_fault)
(227,new_pte)
(228,hugetlb)
(229,mmu_seq = vcpu->kvm->mmu_notifier_seq)
(230,mem_type = PAGE_S2)
(231,*memcache = &vcpu->arch.mmu_page_cache)
(232,userspace_addr)
(233,pmd_mkhuge(new_pmd)
(234,pfn)
(235,struct kvm_memory_slot *memslot)
(236,mem_type)
(237,)
(238,logging_active)
(239,fault_ipa)
(240,PAGE_SHIFT)
(241,memslot)
(242,pfn)
(243,mem_type)
(244,if (hugetlb)
(245,PAGE_SHIFT)
(246,new_pmd)
(247,current->mm)
(248,current)
(249,kvm)
(250,mark_page_dirty(kvm, gfn)
(251,~PMD_MASK)
(252,&writable)
(253,writable)
(254,new_pmd)
(255,hugetlb = transparent_hugepage_adjust(&pfn, &fault_ipa)
(256,hva)
(257,kvm_err("Unexpected L2 read permission error\\n")
(258,force_pte = true)
(259,memslot)
(260,stage2_set_pmd_huge(kvm, memcache, fault_ipa, &new_pmd)
(261,flags |= KVM_S2_FLAG_LOGGING_ACTIVE)
(262,kvm_release_pfn_clean(pfn)
(263,find_vma_intersection(current->mm, hva, hva + 1)
(264,memslot)
(265,memslot->base_gfn << PAGE_SHIFT)
(266,true)
(267,PAGE_SIZE)
(268,kvm)
(269,PAGE_S2_DEVICE)
(270,ret)
(271,fault_ipa)
(272,mmu_topup_memory_cache(memcache, KVM_MMU_CACHE_MIN_PAGES,\n\\n\\t\\t\\t\\t     KVM_NR_MEM_OBJS)
(273,kvm_set_pfn_dirty(pfn)
(274,write_fault)
(275,KVM_S2_FLAG_LOGGING_ACTIVE)
(276,kvm_set_pfn_accessed(pfn)
(277,ret = mmu_topup_memory_cache(memcache, KVM_MMU_CACHE_MIN_PAGES,\n\\n\\t\\t\\t\\t     KVM_NR_MEM_OBJS)
(278,!write_fault)
(279,PMD_SIZE)
(280,kvm_s2pte_mkwrite(new_pte)
(281,force_pte = false)
(282,"Unexpected L2 read permission error\\n")
(283,&current->mm->mmap_sem)
(284,mem_type)
(285,hugetlb)
(286,fault_ipa_uncached = memslot->flags & KVM_MEMSLOT_INCOHERENT)
(287,if (mmu_notifier_retry(kvm, mmu_seq)
(288,pfn)
(289,vcpu->arch)
(290,flags)
(291,ret)
(292,&fault_ipa)
(293,current)
(294,mmu_notifier_retry(kvm, mmu_seq)
(295,mmu_page_cache)
(296,false)
(297,EFAULT)
(298,mem_type)
(299,new_pte = pfn_pte(pfn, mem_type)
(300,memcache)
(301,base_gfn)
(302,mmap_sem)
(303,fault_status == FSC_PERM && !write_fault)
(304,memcache)
(305,gfn)
(306,fault_status == FSC_PERM)
(307,&new_pmd)
(308,memslot)
(309,fault_ipa_uncached)
(310,struct kvm_vcpu *vcpu)
(311,)
(312,mmu_seq)
(313,hva)
(314,RET)
(315,fault_ipa)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-----label-----
1
-----code-----
static int ctr_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,
		       struct scatterlist *src, unsigned int nbytes)
{
	struct crypto_aes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);
	struct blkcipher_walk walk;
	int err, blocks;

	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
	blkcipher_walk_init(&walk, dst, src, nbytes);
	err = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);

	kernel_neon_begin();
	while ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {
		ce_aes_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,
				   (u8 *)ctx->key_enc, num_rounds(ctx), blocks,
				   walk.iv);
		nbytes -= blocks * AES_BLOCK_SIZE;
		if (nbytes && nbytes == walk.nbytes % AES_BLOCK_SIZE)
			break;
		err = blkcipher_walk_done(desc, &walk,
					  walk.nbytes % AES_BLOCK_SIZE);
	}
	if (nbytes) {
		u8 *tdst = walk.dst.virt.addr + blocks * AES_BLOCK_SIZE;
		u8 *tsrc = walk.src.virt.addr + blocks * AES_BLOCK_SIZE;
		u8 __aligned(8) tail[AES_BLOCK_SIZE];

		/*
		 * Minimum alignment is 8 bytes, so if nbytes is <= 8, we need
		 * to tell aes_ctr_encrypt() to only read half a block.
		 */
		blocks = (nbytes <= 8) ? -1 : 1;

		ce_aes_ctr_encrypt(tail, tsrc, (u8 *)ctx->key_enc,
				   num_rounds(ctx), blocks, walk.iv);
		memcpy(tdst, tail, nbytes);
		err = blkcipher_walk_done(desc, &walk, 0);
	}
	kernel_neon_end();

	return err;
}
-----children-----
1,2
1,3
1,4
3,4
3,5
3,6
3,7
3,8
5,6
5,7
6,7
8,9
8,10
11,12
11,13
12,13
14,15
14,16
17,18
17,19
18,19
20,21
20,22
23,24
23,25
25,26
27,28
27,29
27,30
27,31
27,32
27,33
27,34
27,35
27,36
27,37
27,38
28,29
29,30
29,31
30,31
32,33
32,34
32,35
35,36
36,37
36,38
37,38
39,40
39,41
40,41
43,44
44,45
44,46
45,46
47,48
49,50
50,51
50,52
50,53
52,53
54,55
56,57
57,58
57,59
58,59
58,60
59,60
62,63
63,64
65,66
66,67
66,68
66,69
66,70
66,71
67,68
69,70
70,71
72,73
74,75
76,77
78,79
79,80
79,81
80,81
82,83
82,84
82,85
82,86
83,84
85,86
87,88
88,89
90,91
92,93
93,94
94,95
96,97
96,98
97,98
98,99
98,100
99,100
101,102
102,103
102,104
103,104
103,105
104,105
107,108
109,110
109,111
109,112
109,113
110,111
111,112
111,113
111,114
111,115
111,116
111,117
111,118
112,113
114,115
114,116
115,116
115,117
116,117
116,118
117,118
122,123
122,124
123,124
123,125
124,125
124,126
125,126
130,131
130,132
131,132
131,133
132,133
134,135
136,137
136,138
137,138
140,141
140,142
141,142
143,144
145,146
147,148
147,149
148,149
151,152
152,153
152,154
153,154
155,156
155,157
156,157
158,159
160,161
160,162
161,162
161,163
162,163
164,165
164,166
165,166
167,168
167,169
168,169
168,170
169,170
172,173
175,176
176,177
176,178
177,178
179,180
179,181
179,182
179,183
180,181
182,183
184,185
185,186
187,188
187,189
188,189
188,190
189,190
192,193
194,195
194,196
195,196
197,198
197,199
197,200
197,201
197,202
197,203
197,204
197,205
197,206
198,199
199,200
199,201
200,201
202,203
202,204
202,205
205,206
206,207
206,208
207,208
207,209
208,209
208,210
209,210
209,211
210,211
215,216
215,217
216,217
218,219
220,221
221,222
221,223
222,223
224,225
224,226
224,227
227,228
228,229
228,230
229,230
229,231
230,231
230,232
231,232
231,233
232,233
237,238
237,239
238,239
240,241
242,243
243,244
243,245
244,245
246,247
246,248
248,249
250,251
251,252
252,253
252,254
253,254
255,256
257,258
258,259
258,260
259,260
261,262
261,263
261,264
262,263
263,264
263,265
264,265
267,268
270,271
271,272
271,273
271,274
271,275
271,276
271,277
271,278
272,273
274,275
276,277
278,279
278,280
279,280
279,281
280,281
282,283
284,285
284,286
285,286
288,289
288,290
289,290
291,292
293,294
295,296
295,297
296,297
299,300
300,301
300,302
300,303
300,304
301,302
303,304
305,306
307,308
309,310
310,311
310,312
311,312
313,314
313,315
313,316
313,317
314,315
316,317
318,319
319,320
322,323
323,324
324,325
326,327
327,328
-----nextToken-----
2,4,7,9,10,13,15,16,19,21,22,24,26,31,33,34,38,41,42,46,48,51,53,55,60,61,64,68,71,73,75,77,81,84,86,89,91,95,100,105,106,108,113,118,119,120,121,126,127,128,129,133,135,138,139,142,144,146,149,150,154,157,159,163,166,170,171,173,174,178,181,183,186,190,191,193,196,201,203,204,211,212,213,214,217,219,223,225,226,233,234,235,236,239,241,245,247,249,254,256,260,265,266,268,269,273,275,277,281,283,286,287,290,292,294,297,298,302,304,306,308,312,315,317,320,321,325,328
-----computeFrom-----
57,58
57,59
79,80
79,81
98,99
98,100
102,103
102,104
152,153
152,154
155,156
155,157
161,162
161,163
164,165
164,166
167,168
167,169
176,177
176,178
187,188
187,189
206,207
206,208
215,216
215,217
228,229
228,230
237,238
237,239
258,259
258,260
263,264
263,265
310,311
310,312
-----guardedBy-----
-----guardedByNegation-----
-----lastLexicalUse-----
-----jump-----
-----attribute-----
FunctionDefinition;SimpleDeclSpecifier;FunctionDeclarator;Name;ParameterDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;ParameterDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;ParameterDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;ParameterDeclaration;SimpleDeclSpecifier;Declarator;Name;CompoundStatement;DeclarationStatement;SimpleDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;EqualsInitializer;FunctionCallExpression;IdExpression;Name;FieldReference;IdExpression;Name;Name;DeclarationStatement;SimpleDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Name;DeclarationStatement;SimpleDeclaration;SimpleDeclSpecifier;Declarator;Name;Declarator;Name;ExpressionStatement;BinaryExpression;FieldReference;IdExpression;Name;Name;UnaryExpression;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;WhileStatement;UnaryExpression;BinaryExpression;IdExpression;Name;UnaryExpression;BinaryExpression;FieldReference;IdExpression;Name;Name;IdExpression;Name;CompoundStatement;ExpressionStatement;FunctionCallExpression;IdExpression;Name;FieldReference;FieldReference;FieldReference;IdExpression;Name;Name;Name;Name;FieldReference;FieldReference;FieldReference;IdExpression;Name;Name;Name;Name;CastExpression;TypeId;NamedTypeSpecifier;Name;Declarator;Pointer;FieldReference;IdExpression;Name;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;FieldReference;IdExpression;Name;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;BinaryExpression;IdExpression;Name;IdExpression;Name;IfStatement;BinaryExpression;IdExpression;Name;BinaryExpression;IdExpression;Name;BinaryExpression;FieldReference;IdExpression;Name;Name;IdExpression;Name;BreakStatement;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;BinaryExpression;FieldReference;IdExpression;Name;Name;IdExpression;Name;IfStatement;IdExpression;Name;CompoundStatement;DeclarationStatement;SimpleDeclaration;NamedTypeSpecifier;Name;Declarator;Pointer;Name;EqualsInitializer;BinaryExpression;FieldReference;FieldReference;FieldReference;IdExpression;Name;Name;Name;Name;BinaryExpression;IdExpression;Name;IdExpression;Name;DeclarationStatement;SimpleDeclaration;NamedTypeSpecifier;Name;Declarator;Pointer;Name;EqualsInitializer;BinaryExpression;FieldReference;FieldReference;FieldReference;IdExpression;Name;Name;Name;Name;BinaryExpression;IdExpression;Name;IdExpression;Name;DeclarationStatement;SimpleDeclaration;NamedTypeSpecifier;Name;Declarator;Name;ConstructorInitializer;LiteralExpression;ProblemStatement;ExpressionStatement;ArraySubscriptExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;ConditionalExpression;UnaryExpression;BinaryExpression;IdExpression;Name;LiteralExpression;UnaryExpression;LiteralExpression;LiteralExpression;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;CastExpression;TypeId;NamedTypeSpecifier;Name;Declarator;Pointer;FieldReference;IdExpression;Name;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;FieldReference;IdExpression;Name;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;LiteralExpression;ExpressionStatement;FunctionCallExpression;IdExpression;Name;ReturnStatement;IdExpression;Name;
-----ast_node-----
static int ctr_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,		       struct scatterlist *src, unsigned int nbytes){	struct crypto_aes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);	struct blkcipher_walk walk;	int err, blocks;	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;	blkcipher_walk_init(&walk, dst, src, nbytes);	err = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);	kernel_neon_begin();	while ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {		ce_aes_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,				   (u8 *)ctx->key_enc, num_rounds(ctx), blocks,				   walk.iv);		nbytes -= blocks * AES_BLOCK_SIZE;		if (nbytes && nbytes == walk.nbytes % AES_BLOCK_SIZE)			break;		err = blkcipher_walk_done(desc, &walk,					  walk.nbytes % AES_BLOCK_SIZE);	}	if (nbytes) {		u8 *tdst = walk.dst.virt.addr + blocks * AES_BLOCK_SIZE;		u8 *tsrc = walk.src.virt.addr + blocks * AES_BLOCK_SIZE;		u8 __aligned(8) tail[AES_BLOCK_SIZE];		/*		 * Minimum alignment is 8 bytes, so if nbytes is <= 8, we need		 * to tell aes_ctr_encrypt() to only read half a block.		 */		blocks = (nbytes <= 8) ? -1 : 1;		ce_aes_ctr_encrypt(tail, tsrc, (u8 *)ctx->key_enc,				   num_rounds(ctx), blocks, walk.iv);		memcpy(tdst, tail, nbytes);		err = blkcipher_walk_done(desc, &walk, 0);	}	kernel_neon_end();	return err;}
static int
ctr_encrypt(struct blkcipher_desc *desc, struct scatterlist *dst,		       struct scatterlist *src, unsigned int nbytes)
ctr_encrypt
struct blkcipher_desc *desc
struct blkcipher_desc
blkcipher_desc
*desc
*
desc
struct scatterlist *dst
struct scatterlist
scatterlist
*dst
*
dst
struct scatterlist *src
struct scatterlist
scatterlist
*src
*
src
unsigned int nbytes
unsigned int
nbytes
nbytes
{	struct crypto_aes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);	struct blkcipher_walk walk;	int err, blocks;	desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;	blkcipher_walk_init(&walk, dst, src, nbytes);	err = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);	kernel_neon_begin();	while ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {		ce_aes_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,				   (u8 *)ctx->key_enc, num_rounds(ctx), blocks,				   walk.iv);		nbytes -= blocks * AES_BLOCK_SIZE;		if (nbytes && nbytes == walk.nbytes % AES_BLOCK_SIZE)			break;		err = blkcipher_walk_done(desc, &walk,					  walk.nbytes % AES_BLOCK_SIZE);	}	if (nbytes) {		u8 *tdst = walk.dst.virt.addr + blocks * AES_BLOCK_SIZE;		u8 *tsrc = walk.src.virt.addr + blocks * AES_BLOCK_SIZE;		u8 __aligned(8) tail[AES_BLOCK_SIZE];		/*		 * Minimum alignment is 8 bytes, so if nbytes is <= 8, we need		 * to tell aes_ctr_encrypt() to only read half a block.		 */		blocks = (nbytes <= 8) ? -1 : 1;		ce_aes_ctr_encrypt(tail, tsrc, (u8 *)ctx->key_enc,				   num_rounds(ctx), blocks, walk.iv);		memcpy(tdst, tail, nbytes);		err = blkcipher_walk_done(desc, &walk, 0);	}	kernel_neon_end();	return err;}
struct crypto_aes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);
struct crypto_aes_ctx *ctx = crypto_blkcipher_ctx(desc->tfm);
struct crypto_aes_ctx
crypto_aes_ctx
*ctx = crypto_blkcipher_ctx(desc->tfm)
*
ctx
= crypto_blkcipher_ctx(desc->tfm)
crypto_blkcipher_ctx(desc->tfm)
crypto_blkcipher_ctx
crypto_blkcipher_ctx
desc->tfm
desc
desc
tfm
struct blkcipher_walk walk;
struct blkcipher_walk walk;
struct blkcipher_walk
blkcipher_walk
walk
walk
int err, blocks;
int err, blocks;
int
err
err
blocks
blocks
desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP;
desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP
desc->flags
desc
desc
flags
~CRYPTO_TFM_REQ_MAY_SLEEP
CRYPTO_TFM_REQ_MAY_SLEEP
CRYPTO_TFM_REQ_MAY_SLEEP
blkcipher_walk_init(&walk, dst, src, nbytes);
blkcipher_walk_init(&walk, dst, src, nbytes)
blkcipher_walk_init
blkcipher_walk_init
&walk
walk
walk
dst
dst
src
src
nbytes
nbytes
err = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE);
err = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE)
err
err
blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE)
blkcipher_walk_virt_block
blkcipher_walk_virt_block
desc
desc
&walk
walk
walk
AES_BLOCK_SIZE
AES_BLOCK_SIZE
kernel_neon_begin();
kernel_neon_begin()
kernel_neon_begin
kernel_neon_begin
while ((blocks = (walk.nbytes / AES_BLOCK_SIZE))) {		ce_aes_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,				   (u8 *)ctx->key_enc, num_rounds(ctx), blocks,				   walk.iv);		nbytes -= blocks * AES_BLOCK_SIZE;		if (nbytes && nbytes == walk.nbytes % AES_BLOCK_SIZE)			break;		err = blkcipher_walk_done(desc, &walk,					  walk.nbytes % AES_BLOCK_SIZE);	}
(blocks = (walk.nbytes / AES_BLOCK_SIZE))
blocks = (walk.nbytes / AES_BLOCK_SIZE)
blocks
blocks
(walk.nbytes / AES_BLOCK_SIZE)
walk.nbytes / AES_BLOCK_SIZE
walk.nbytes
walk
walk
nbytes
AES_BLOCK_SIZE
AES_BLOCK_SIZE
{		ce_aes_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,				   (u8 *)ctx->key_enc, num_rounds(ctx), blocks,				   walk.iv);		nbytes -= blocks * AES_BLOCK_SIZE;		if (nbytes && nbytes == walk.nbytes % AES_BLOCK_SIZE)			break;		err = blkcipher_walk_done(desc, &walk,					  walk.nbytes % AES_BLOCK_SIZE);	}
ce_aes_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,				   (u8 *)ctx->key_enc, num_rounds(ctx), blocks,				   walk.iv);
ce_aes_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,				   (u8 *)ctx->key_enc, num_rounds(ctx), blocks,				   walk.iv)
ce_aes_ctr_encrypt
ce_aes_ctr_encrypt
walk.dst.virt.addr
walk.dst.virt
walk.dst
walk
walk
dst
virt
addr
walk.src.virt.addr
walk.src.virt
walk.src
walk
walk
src
virt
addr
(u8 *)ctx->key_enc
u8 *
u8
u8
*
*
ctx->key_enc
ctx
ctx
key_enc
num_rounds(ctx)
num_rounds
num_rounds
ctx
ctx
blocks
blocks
walk.iv
walk
walk
iv
nbytes -= blocks * AES_BLOCK_SIZE;
nbytes -= blocks * AES_BLOCK_SIZE
nbytes
nbytes
blocks * AES_BLOCK_SIZE
blocks
blocks
AES_BLOCK_SIZE
AES_BLOCK_SIZE
if (nbytes && nbytes == walk.nbytes % AES_BLOCK_SIZE)			break;
nbytes && nbytes == walk.nbytes % AES_BLOCK_SIZE
nbytes
nbytes
nbytes == walk.nbytes % AES_BLOCK_SIZE
nbytes
nbytes
walk.nbytes % AES_BLOCK_SIZE
walk.nbytes
walk
walk
nbytes
AES_BLOCK_SIZE
AES_BLOCK_SIZE
break;
err = blkcipher_walk_done(desc, &walk,					  walk.nbytes % AES_BLOCK_SIZE);
err = blkcipher_walk_done(desc, &walk,					  walk.nbytes % AES_BLOCK_SIZE)
err
err
blkcipher_walk_done(desc, &walk,					  walk.nbytes % AES_BLOCK_SIZE)
blkcipher_walk_done
blkcipher_walk_done
desc
desc
&walk
walk
walk
walk.nbytes % AES_BLOCK_SIZE
walk.nbytes
walk
walk
nbytes
AES_BLOCK_SIZE
AES_BLOCK_SIZE
if (nbytes) {		u8 *tdst = walk.dst.virt.addr + blocks * AES_BLOCK_SIZE;		u8 *tsrc = walk.src.virt.addr + blocks * AES_BLOCK_SIZE;		u8 __aligned(8) tail[AES_BLOCK_SIZE];		/*		 * Minimum alignment is 8 bytes, so if nbytes is <= 8, we need		 * to tell aes_ctr_encrypt() to only read half a block.		 */		blocks = (nbytes <= 8) ? -1 : 1;		ce_aes_ctr_encrypt(tail, tsrc, (u8 *)ctx->key_enc,				   num_rounds(ctx), blocks, walk.iv);		memcpy(tdst, tail, nbytes);		err = blkcipher_walk_done(desc, &walk, 0);	}
nbytes
nbytes
{		u8 *tdst = walk.dst.virt.addr + blocks * AES_BLOCK_SIZE;		u8 *tsrc = walk.src.virt.addr + blocks * AES_BLOCK_SIZE;		u8 __aligned(8) tail[AES_BLOCK_SIZE];		/*		 * Minimum alignment is 8 bytes, so if nbytes is <= 8, we need		 * to tell aes_ctr_encrypt() to only read half a block.		 */		blocks = (nbytes <= 8) ? -1 : 1;		ce_aes_ctr_encrypt(tail, tsrc, (u8 *)ctx->key_enc,				   num_rounds(ctx), blocks, walk.iv);		memcpy(tdst, tail, nbytes);		err = blkcipher_walk_done(desc, &walk, 0);	}
u8 *tdst = walk.dst.virt.addr + blocks * AES_BLOCK_SIZE;
u8 *tdst = walk.dst.virt.addr + blocks * AES_BLOCK_SIZE;
u8
u8
*tdst = walk.dst.virt.addr + blocks * AES_BLOCK_SIZE
*
tdst
= walk.dst.virt.addr + blocks * AES_BLOCK_SIZE
walk.dst.virt.addr + blocks * AES_BLOCK_SIZE
walk.dst.virt.addr
walk.dst.virt
walk.dst
walk
walk
dst
virt
addr
blocks * AES_BLOCK_SIZE
blocks
blocks
AES_BLOCK_SIZE
AES_BLOCK_SIZE
u8 *tsrc = walk.src.virt.addr + blocks * AES_BLOCK_SIZE;
u8 *tsrc = walk.src.virt.addr + blocks * AES_BLOCK_SIZE;
u8
u8
*tsrc = walk.src.virt.addr + blocks * AES_BLOCK_SIZE
*
tsrc
= walk.src.virt.addr + blocks * AES_BLOCK_SIZE
walk.src.virt.addr + blocks * AES_BLOCK_SIZE
walk.src.virt.addr
walk.src.virt
walk.src
walk
walk
src
virt
addr
blocks * AES_BLOCK_SIZE
blocks
blocks
AES_BLOCK_SIZE
AES_BLOCK_SIZE
u8 __aligned(8)
u8 __aligned(8)
u8
u8
__aligned(8)
__aligned
(8)
8
)
tail[AES_BLOCK_SIZE];
tail[AES_BLOCK_SIZE]
tail
tail
AES_BLOCK_SIZE
AES_BLOCK_SIZE
blocks = (nbytes <= 8) ? -1 : 1;
blocks = (nbytes <= 8) ? -1 : 1
blocks
blocks
(nbytes <= 8) ? -1 : 1
(nbytes <= 8)
nbytes <= 8
nbytes
nbytes
8
-1
1
1
ce_aes_ctr_encrypt(tail, tsrc, (u8 *)ctx->key_enc,				   num_rounds(ctx), blocks, walk.iv);
ce_aes_ctr_encrypt(tail, tsrc, (u8 *)ctx->key_enc,				   num_rounds(ctx), blocks, walk.iv)
ce_aes_ctr_encrypt
ce_aes_ctr_encrypt
tail
tail
tsrc
tsrc
(u8 *)ctx->key_enc
u8 *
u8
u8
*
*
ctx->key_enc
ctx
ctx
key_enc
num_rounds(ctx)
num_rounds
num_rounds
ctx
ctx
blocks
blocks
walk.iv
walk
walk
iv
memcpy(tdst, tail, nbytes);
memcpy(tdst, tail, nbytes)
memcpy
memcpy
tdst
tdst
tail
tail
nbytes
nbytes
err = blkcipher_walk_done(desc, &walk, 0);
err = blkcipher_walk_done(desc, &walk, 0)
err
err
blkcipher_walk_done(desc, &walk, 0)
blkcipher_walk_done
blkcipher_walk_done
desc
desc
&walk
walk
walk
0
kernel_neon_end();
kernel_neon_end()
kernel_neon_end
kernel_neon_end
return err;
err
err
-----joern-----
(148,16,0)
(113,45,0)
(23,137,0)
(65,88,0)
(8,87,0)
(16,9,0)
(135,147,0)
(15,124,0)
(113,147,0)
(37,98,0)
(21,8,0)
(141,146,0)
(128,68,0)
(33,143,0)
(120,65,0)
(121,10,0)
(123,59,0)
(51,148,0)
(112,150,0)
(106,145,0)
(107,28,0)
(129,115,0)
(18,68,0)
(154,57,0)
(166,48,0)
(31,144,0)
(133,48,0)
(16,148,0)
(69,57,0)
(7,166,0)
(65,18,0)
(29,14,0)
(27,57,0)
(48,0,0)
(136,160,0)
(4,115,0)
(52,161,0)
(32,119,0)
(108,43,0)
(68,18,0)
(3,147,0)
(98,115,0)
(24,12,0)
(17,115,0)
(139,72,0)
(163,124,0)
(98,16,0)
(127,59,0)
(55,108,0)
(48,16,0)
(71,161,0)
(50,44,0)
(23,94,0)
(93,17,0)
(20,106,0)
(154,125,0)
(24,26,0)
(162,145,0)
(129,113,0)
(66,60,0)
(33,94,0)
(146,115,0)
(153,164,0)
(72,161,0)
(130,23,0)
(69,14,0)
(109,45,0)
(146,17,0)
(155,23,0)
(75,13,0)
(11,24,0)
(131,161,0)
(101,161,0)
(12,115,0)
(72,102,0)
(59,123,0)
(122,123,0)
(167,162,0)
(109,5,0)
(48,166,0)
(85,143,0)
(149,69,0)
(23,88,0)
(45,109,0)
(43,146,0)
(116,89,0)
(167,69,0)
(146,150,0)
(146,160,0)
(33,12,0)
(146,43,0)
(118,28,0)
(53,64,0)
(83,72,0)
(84,108,0)
(108,145,0)
(156,146,0)
(77,98,0)
(26,24,0)
(164,153,0)
(113,129,0)
(134,121,0)
(36,72,0)
(70,89,0)
(2,148,0)
(152,161,0)
(69,106,0)
(69,9,0)
(99,161,0)
(45,113,0)
(40,123,0)
(94,161,0)
(16,0,0)
(10,144,0)
(151,115,0)
(92,164,0)
(165,161,0)
(162,167,0)
(44,16,0)
(140,5,0)
(100,48,0)
(80,33,0)
(122,153,0)
(117,154,0)
(167,81,0)
(64,16,0)
(148,87,0)
(132,26,0)
(69,87,0)
(19,87,0)
(159,16,0)
(72,157,0)
(8,21,0)
(34,88,0)
(98,129,0)
(105,161,0)
(126,137,0)
(91,10,0)
(144,10,0)
(90,98,0)
(47,165,0)
(13,72,0)
(69,167,0)
(76,94,0)
(81,167,0)
(57,154,0)
(64,44,0)
(42,125,0)
(160,146,0)
(43,95,0)
(69,144,0)
(89,60,0)
(143,65,0)
(153,122,0)
(72,13,0)
(65,97,0)
(74,125,0)
(160,145,0)
(16,16,0)
(94,23,0)
(144,69,0)
(150,146,0)
(56,146,0)
(124,87,0)
(25,145,0)
(49,9,0)
(73,8,0)
(43,108,0)
(72,0,0)
(46,145,0)
(62,121,0)
(30,119,0)
(110,14,0)
(54,109,0)
(123,122,0)
(138,45,0)
(41,122,0)
(25,34,0)
(103,162,0)
(64,60,0)
(34,25,0)
(78,166,0)
(18,65,0)
(147,113,0)
(125,154,0)
(57,69,0)
(119,165,0)
(146,164,0)
(147,87,0)
(166,17,0)
(164,115,0)
(10,121,0)
(142,129,0)
(61,5,0)
(87,8,0)
(97,65,0)
(166,78,0)
(95,43,0)
(35,68,0)
(79,97,0)
(114,21,0)
(17,166,0)
(1,18,0)
(145,25,0)
(6,78,0)
(64,0,0)
(111,21,0)
(22,33,0)
(137,23,0)
(39,34,0)
(86,24,0)
(158,162,0)
(44,0,0)
(106,69,0)
(28,115,0)
(98,0,0)
(14,69,0)
(5,109,0)
(104,59,0)
(58,150,0)
(12,24,0)
(143,9,0)
(119,88,0)
(145,161,0)
(38,161,0)
(60,89,0)
(165,119,0)
(44,64,0)
(60,64,0)
(24,88,0)
(65,143,0)
(153,124,0)
(33,161,0)
(82,34,0)
(124,153,0)
(63,12,0)
(56,17,1)
(3,135,1)
(16,16,1)
(109,5,1)
(69,57,1)
(148,87,1)
(152,12,1)
(165,47,1)
(23,88,1)
(166,7,1)
(19,38,1)
(108,55,1)
(16,0,1)
(24,26,1)
(97,79,1)
(153,122,1)
(44,64,1)
(119,32,1)
(16,148,1)
(87,8,1)
(48,16,1)
(90,77,1)
(53,50,1)
(60,66,1)
(167,162,1)
(44,50,1)
(48,133,1)
(7,48,1)
(24,86,1)
(166,48,1)
(64,60,1)
(148,51,1)
(78,6,1)
(104,92,1)
(37,146,1)
(32,30,1)
(82,46,1)
(10,91,1)
(72,157,1)
(94,23,1)
(142,67,1)
(13,75,1)
(6,48,1)
(68,35,1)
(79,120,1)
(15,163,1)
(64,16,1)
(130,137,1)
(123,40,1)
(113,147,1)
(65,88,1)
(65,97,1)
(123,59,1)
(153,124,1)
(21,111,1)
(156,160,1)
(44,16,1)
(40,59,1)
(154,125,1)
(126,155,1)
(98,129,1)
(28,107,1)
(152,67,1)
(122,41,1)
(30,145,1)
(127,104,1)
(144,31,1)
(33,94,1)
(69,144,1)
(146,164,1)
(158,103,1)
(24,88,1)
(150,58,1)
(29,110,1)
(57,27,1)
(118,164,1)
(149,106,1)
(146,43,1)
(119,88,1)
(124,87,1)
(138,109,1)
(55,84,1)
(62,87,1)
(96,44,1)
(147,3,1)
(65,18,1)
(103,57,1)
(72,0,1)
(146,150,1)
(113,45,1)
(110,149,1)
(155,76,1)
(125,42,1)
(41,123,1)
(86,26,1)
(47,119,1)
(74,144,1)
(45,109,1)
(137,126,1)
(18,1,1)
(160,145,1)
(89,70,1)
(34,39,1)
(57,154,1)
(50,16,1)
(93,28,1)
(117,125,1)
(23,130,1)
(69,167,1)
(146,17,1)
(162,158,1)
(44,0,1)
(85,44,1)
(154,117,1)
(27,154,1)
(64,0,1)
(18,68,1)
(133,100,1)
(77,37,1)
(54,5,1)
(72,36,1)
(42,74,1)
(25,34,1)
(17,166,1)
(147,87,1)
(73,21,1)
(31,10,1)
(34,88,1)
(83,139,1)
(114,19,1)
(159,69,1)
(36,83,1)
(163,122,1)
(167,81,1)
(106,145,1)
(75,165,1)
(69,87,1)
(116,53,1)
(72,102,1)
(66,89,1)
(98,90,1)
(2,159,1)
(139,13,1)
(141,56,1)
(109,54,1)
(164,153,1)
(26,132,1)
(33,12,1)
(107,118,1)
(10,121,1)
(58,112,1)
(128,97,1)
(69,14,1)
(20,167,1)
(136,43,1)
(61,142,1)
(12,24,1)
(134,62,1)
(144,10,1)
(48,0,1)
(33,143,1)
(106,20,1)
(70,116,1)
(59,127,1)
(5,140,1)
(43,95,1)
(72,13,1)
(112,156,1)
(162,145,1)
(80,152,1)
(145,25,1)
(143,65,1)
(122,123,1)
(1,68,1)
(22,33,1)
(33,80,1)
(111,114,1)
(120,85,1)
(76,72,1)
(108,145,1)
(124,15,1)
(135,45,1)
(91,121,1)
(43,108,1)
(100,93,1)
(140,61,1)
(84,141,1)
(67,96,1)
(98,16,1)
(45,138,1)
(11,63,1)
(129,113,1)
(67,87,1)
(165,119,1)
(19,143,1)
(8,21,1)
(51,2,1)
(60,89,1)
(38,94,1)
(92,129,1)
(8,73,1)
(132,11,1)
(14,29,1)
(39,82,1)
(69,106,1)
(121,134,1)
(98,0,1)
(166,78,1)
(23,137,1)
(160,136,1)
(35,128,1)
(146,160,1)
(63,98,1)
(140,67,2)
(21,44,2)
(16,148,2)
(91,87,2)
(2,87,2)
(149,87,2)
(64,0,2)
(144,10,2)
(43,108,2)
(64,50,2)
(44,0,2)
(48,67,2)
(136,67,2)
(111,44,2)
(119,88,2)
(113,147,2)
(69,167,2)
(167,81,2)
(44,16,2)
(98,0,2)
(7,48,2)
(64,60,2)
(55,67,2)
(109,67,2)
(85,44,2)
(65,97,2)
(58,67,2)
(60,50,2)
(167,162,2)
(73,44,2)
(124,87,2)
(129,113,2)
(12,67,2)
(77,67,2)
(96,44,2)
(69,106,2)
(43,67,2)
(129,67,2)
(113,45,2)
(146,17,2)
(108,145,2)
(56,67,2)
(94,23,2)
(84,67,2)
(87,8,2)
(98,129,2)
(8,44,2)
(142,67,2)
(141,67,2)
(122,67,2)
(128,44,2)
(166,67,2)
(127,67,2)
(42,87,2)
(147,87,2)
(134,87,2)
(23,137,2)
(33,143,2)
(44,64,2)
(18,68,2)
(146,164,2)
(112,67,2)
(120,44,2)
(31,87,2)
(63,67,2)
(69,14,2)
(158,87,2)
(3,67,2)
(146,160,2)
(59,67,2)
(54,67,2)
(34,88,2)
(113,67,2)
(65,88,2)
(132,67,2)
(108,67,2)
(23,88,2)
(162,145,2)
(93,67,2)
(167,87,2)
(153,122,2)
(153,67,2)
(148,87,2)
(72,0,2)
(16,16,2)
(109,5,2)
(45,67,2)
(8,21,2)
(160,145,2)
(24,26,2)
(51,87,2)
(106,145,2)
(69,144,2)
(64,16,2)
(123,67,2)
(24,88,2)
(164,153,2)
(79,44,2)
(11,67,2)
(72,102,2)
(146,150,2)
(153,124,2)
(144,87,2)
(146,67,2)
(122,123,2)
(162,87,2)
(106,87,2)
(48,0,2)
(118,67,2)
(78,48,2)
(92,67,2)
(98,67,2)
(40,67,2)
(45,109,2)
(114,44,2)
(10,121,2)
(107,67,2)
(159,87,2)
(24,67,2)
(16,0,2)
(61,67,2)
(43,95,2)
(17,67,2)
(37,67,2)
(154,87,2)
(125,87,2)
(10,87,2)
(86,67,2)
(156,67,2)
(138,67,2)
(72,13,2)
(160,67,2)
(33,94,2)
(145,25,2)
(103,87,2)
(20,87,2)
(26,67,2)
(87,44,2)
(154,125,2)
(100,67,2)
(62,87,2)
(6,48,2)
(90,67,2)
(65,44,2)
(69,87,2)
(1,44,2)
(143,44,2)
(27,87,2)
(123,59,2)
(28,67,2)
(166,78,2)
(70,50,2)
(150,67,2)
(60,89,2)
(110,87,2)
(72,157,2)
(22,33,2)
(163,67,2)
(33,12,2)
(133,67,2)
(117,87,2)
(53,50,2)
(41,67,2)
(74,87,2)
(166,48,2)
(35,44,2)
(25,34,2)
(165,119,2)
(18,44,2)
(50,87,2)
(146,43,2)
(48,16,2)
(143,65,2)
(69,57,2)
(44,87,2)
(98,16,2)
(116,50,2)
(12,24,2)
(16,87,2)
(66,50,2)
(164,67,2)
(147,67,2)
(5,67,2)
(97,44,2)
(65,18,2)
(121,87,2)
(14,87,2)
(19,44,2)
(89,50,2)
(15,67,2)
(29,87,2)
(104,67,2)
(57,87,2)
(124,67,2)
(57,154,2)
(17,166,2)
(135,67,2)
(68,44,2)
-----------------------------------
(0,unsigned int nbytes)
(1,AES_BLOCK_SIZE)
(2,blocks)
(3,AES_BLOCK_SIZE)
(4,tdst)
(5,walk.dst)
(6,1)
(7,1)
(8,walk.nbytes / AES_BLOCK_SIZE)
(9,)
(10,walk.dst.virt)
(11,desc)
(12,err = blkcipher_walk_done(desc, &walk, 0)
(13,&walk)
(14,walk.iv)
(15,AES_BLOCK_SIZE)
(16,nbytes -= blocks * AES_BLOCK_SIZE)
(17,blocks = (nbytes <= 8)
(18,walk.nbytes % AES_BLOCK_SIZE)
(19,blocks)
(20,ctx)
(21,walk.nbytes)
(22,RET)
(23,blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE)
(24,blkcipher_walk_done(desc, &walk, 0)
(25,crypto_blkcipher_ctx(desc->tfm)
(26,&walk)
(27,addr)
(28,tail[AES_BLOCK_SIZE])
(29,iv)
(30,desc)
(31,addr)
(32,flags)
(33,return err;)
(34,desc->tfm)
(35,nbytes)
(36,nbytes)
(37,tdst)
(38,kernel_neon_begin()
(39,tfm)
(40,virt)
(41,addr)
(42,src)
(43,(u8 *)
(44,nbytes && nbytes == walk.nbytes % AES_BLOCK_SIZE)
(45,walk.dst.virt.addr)
(46,ctx)
(47,~CRYPTO_TFM_REQ_MAY_SLEEP)
(48,nbytes <= 8)
(49,if (nbytes && nbytes == walk.nbytes % AES_BLOCK_SIZE)
(50,nbytes)
(51,AES_BLOCK_SIZE)
(52,walk)
(53,nbytes)
(54,virt)
(55,key_enc)
(56,tail)
(57,walk.src.virt.addr)
(58,iv)
(59,walk.src)
(60,walk.nbytes % AES_BLOCK_SIZE)
(61,walk)
(62,walk)
(63,err)
(64,nbytes == walk.nbytes % AES_BLOCK_SIZE)
(65,blkcipher_walk_done(desc, &walk,\n\\n\\t\\t\\t\\t\\t  walk.nbytes % AES_BLOCK_SIZE)
(66,AES_BLOCK_SIZE)
(67,nbytes)
(68,walk.nbytes)
(69,ce_aes_ctr_encrypt(walk.dst.virt.addr, walk.src.virt.addr,\n\\n\\t\\t\\t\\t   (u8 *)
(70,nbytes)
(71,while ((blocks = (walk.nbytes / AES_BLOCK_SIZE)
(72,blkcipher_walk_init(&walk, dst, src, nbytes)
(73,AES_BLOCK_SIZE)
(74,walk)
(75,walk)
(76,err)
(77,tail)
(78,-1)
(79,walk)
(80,err)
(81,u8 *)
(82,desc)
(83,src)
(84,ctx)
(85,err)
(86,0)
(87,blocks = (walk.nbytes / AES_BLOCK_SIZE)
(88,struct blkcipher_desc *desc)
(89,walk.nbytes)
(90,nbytes)
(91,virt)
(92,tsrc)
(93,blocks)
(94,err = blkcipher_walk_virt_block(desc, &walk, AES_BLOCK_SIZE)
(95,u8 *)
(96,break;)
(97,&walk)
(98,memcpy(tdst, tail, nbytes)
(99,ctx)
(100,nbytes)
(101,err)
(102,struct scatterlist *dst)
(103,ctx)
(104,walk)
(105,if (nbytes)
(106,num_rounds(ctx)
(107,AES_BLOCK_SIZE)
(108,ctx->key_enc)
(109,walk.dst.virt)
(110,walk)
(111,nbytes)
(112,walk)
(113,walk.dst.virt.addr + blocks * AES_BLOCK_SIZE)
(114,walk)
(115,)
(116,walk)
(117,virt)
(118,tail)
(119,desc->flags)
(120,desc)
(121,walk.dst)
(122,walk.src.virt.addr)
(123,walk.src.virt)
(124,blocks * AES_BLOCK_SIZE)
(125,walk.src)
(126,walk)
(127,src)
(128,walk)
(129,*tdst = walk.dst.virt.addr + blocks * AES_BLOCK_SIZE)
(130,AES_BLOCK_SIZE)
(131,blocks)
(132,walk)
(133,8)
(134,dst)
(135,blocks)
(136,ctx)
(137,&walk)
(138,addr)
(139,dst)
(140,dst)
(141,tsrc)
(142,tdst)
(143,err = blkcipher_walk_done(desc, &walk,\n\\n\\t\\t\\t\\t\\t  walk.nbytes % AES_BLOCK_SIZE)
(144,walk.dst.virt.addr)
(145,*ctx = crypto_blkcipher_ctx(desc->tfm)
(146,ce_aes_ctr_encrypt(tail, tsrc, (u8 *)
(147,blocks * AES_BLOCK_SIZE)
(148,blocks * AES_BLOCK_SIZE)
(149,blocks)
(150,walk.iv)
(151,tsrc)
(152,kernel_neon_end()
(153,walk.src.virt.addr + blocks * AES_BLOCK_SIZE)
(154,walk.src.virt)
(155,desc)
(156,blocks)
(157,struct scatterlist *src)
(158,key_enc)
(159,nbytes)
(160,num_rounds(ctx)
(161,)
(162,ctx->key_enc)
(163,blocks)
(164,*tsrc = walk.src.virt.addr + blocks * AES_BLOCK_SIZE)
(165,desc->flags &= ~CRYPTO_TFM_REQ_MAY_SLEEP)
(166,(nbytes <= 8)
(167,(u8 *)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
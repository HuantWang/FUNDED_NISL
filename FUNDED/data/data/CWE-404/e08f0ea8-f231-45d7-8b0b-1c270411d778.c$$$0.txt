-----label-----
0
-----code-----
static int stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache
			       *cache, phys_addr_t addr, const pmd_t *new_pmd)
{
	pmd_t *pmd, old_pmd;

	pmd = stage2_get_pmd(kvm, cache, addr);
	VM_BUG_ON(!pmd);

	/*
	 * Mapping in huge pages should only happen through a fault.  If a
	 * page is merged into a transparent huge page, the individual
	 * subpages of that huge page should be unmapped through MMU
	 * notifiers before we get here.
	 *
	 * Merging of CompoundPages is not supported; they should become
	 * splitting first, unmapped, merged, and mapped back in on-demand.
	 */
	VM_BUG_ON(pmd_present(*pmd) && pmd_pfn(*pmd) != pmd_pfn(*new_pmd));

	old_pmd = *pmd;
	if (pmd_present(old_pmd)) {
		pmd_clear(pmd);
		kvm_tlb_flush_vmid_ipa(kvm, addr);
	} else {
		get_page(virt_to_page(pmd));
	}

	kvm_set_pmd(pmd, *new_pmd);
	return 0;
}
-----children-----
1,2
1,3
1,4
3,4
3,5
3,6
3,7
3,8
5,6
5,7
6,7
8,9
8,10
11,12
11,13
12,13
14,15
14,16
17,18
17,19
18,19
20,21
22,23
22,24
23,24
25,26
25,27
28,29
28,30
28,31
28,32
28,33
28,34
28,35
28,36
29,30
30,31
30,32
30,33
31,32
33,34
33,35
36,37
38,39
39,40
39,41
40,41
42,43
42,44
42,45
42,46
43,44
45,46
47,48
49,50
51,52
52,53
52,54
53,54
55,56
56,57
58,59
59,60
59,61
60,61
62,63
62,64
63,64
63,65
64,65
66,67
67,68
69,70
69,71
70,71
70,72
71,72
73,74
74,75
76,77
76,78
77,78
79,80
80,81
82,83
83,84
83,85
84,85
86,87
87,88
89,90
89,91
89,92
90,91
90,92
91,92
93,94
95,96
95,97
96,97
97,98
97,99
98,99
100,101
102,103
103,104
103,105
103,106
104,105
106,107
108,109
110,111
111,112
112,113
112,114
113,114
115,116
115,117
116,117
118,119
120,121
121,122
121,123
121,124
122,123
124,125
126,127
127,128
129,130
-----nextToken-----
2,4,7,9,10,13,15,16,19,21,24,26,27,32,34,35,37,41,44,46,48,50,54,57,61,65,68,72,75,78,81,85,88,92,94,99,101,105,107,109,114,117,119,123,125,128,130
-----computeFrom-----
39,40
39,41
62,63
62,64
69,70
69,71
83,84
83,85
-----guardedBy-----
-----guardedByNegation-----
-----lastLexicalUse-----
-----jump-----
-----attribute-----
FunctionDefinition;SimpleDeclSpecifier;FunctionDeclarator;Name;ParameterDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;ParameterDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;ParameterDeclaration;NamedTypeSpecifier;Name;Declarator;Name;ParameterDeclaration;NamedTypeSpecifier;Name;Declarator;Pointer;Name;CompoundStatement;DeclarationStatement;SimpleDeclaration;NamedTypeSpecifier;Name;Declarator;Pointer;Name;Declarator;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;BinaryExpression;FunctionCallExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;BinaryExpression;FunctionCallExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;IfStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;CompoundStatement;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;CompoundStatement;ExpressionStatement;FunctionCallExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;ReturnStatement;LiteralExpression;
-----ast_node-----
static int stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache			       *cache, phys_addr_t addr, const pmd_t *new_pmd){	pmd_t *pmd, old_pmd;	pmd = stage2_get_pmd(kvm, cache, addr);	VM_BUG_ON(!pmd);	/*	 * Mapping in huge pages should only happen through a fault.  If a	 * page is merged into a transparent huge page, the individual	 * subpages of that huge page should be unmapped through MMU	 * notifiers before we get here.	 *	 * Merging of CompoundPages is not supported; they should become	 * splitting first, unmapped, merged, and mapped back in on-demand.	 */	VM_BUG_ON(pmd_present(*pmd) && pmd_pfn(*pmd) != pmd_pfn(*new_pmd));	old_pmd = *pmd;	if (pmd_present(old_pmd)) {		pmd_clear(pmd);		kvm_tlb_flush_vmid_ipa(kvm, addr);	} else {		get_page(virt_to_page(pmd));	}	kvm_set_pmd(pmd, *new_pmd);	return 0;}
static int
stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache			       *cache, phys_addr_t addr, const pmd_t *new_pmd)
stage2_set_pmd_huge
struct kvm *kvm
struct kvm
kvm
*kvm
*
kvm
struct kvm_mmu_memory_cache			       *cache
struct kvm_mmu_memory_cache
kvm_mmu_memory_cache
*cache
*
cache
phys_addr_t addr
phys_addr_t
phys_addr_t
addr
addr
const pmd_t *new_pmd
const pmd_t
pmd_t
*new_pmd
*
new_pmd
{	pmd_t *pmd, old_pmd;	pmd = stage2_get_pmd(kvm, cache, addr);	VM_BUG_ON(!pmd);	/*	 * Mapping in huge pages should only happen through a fault.  If a	 * page is merged into a transparent huge page, the individual	 * subpages of that huge page should be unmapped through MMU	 * notifiers before we get here.	 *	 * Merging of CompoundPages is not supported; they should become	 * splitting first, unmapped, merged, and mapped back in on-demand.	 */	VM_BUG_ON(pmd_present(*pmd) && pmd_pfn(*pmd) != pmd_pfn(*new_pmd));	old_pmd = *pmd;	if (pmd_present(old_pmd)) {		pmd_clear(pmd);		kvm_tlb_flush_vmid_ipa(kvm, addr);	} else {		get_page(virt_to_page(pmd));	}	kvm_set_pmd(pmd, *new_pmd);	return 0;}
pmd_t *pmd, old_pmd;
pmd_t *pmd, old_pmd;
pmd_t
pmd_t
*pmd
*
pmd
old_pmd
old_pmd
pmd = stage2_get_pmd(kvm, cache, addr);
pmd = stage2_get_pmd(kvm, cache, addr)
pmd
pmd
stage2_get_pmd(kvm, cache, addr)
stage2_get_pmd
stage2_get_pmd
kvm
kvm
cache
cache
addr
addr
VM_BUG_ON(!pmd);
VM_BUG_ON(!pmd)
VM_BUG_ON
VM_BUG_ON
!pmd
pmd
pmd
VM_BUG_ON(pmd_present(*pmd) && pmd_pfn(*pmd) != pmd_pfn(*new_pmd));
VM_BUG_ON(pmd_present(*pmd) && pmd_pfn(*pmd) != pmd_pfn(*new_pmd))
VM_BUG_ON
VM_BUG_ON
pmd_present(*pmd) && pmd_pfn(*pmd) != pmd_pfn(*new_pmd)
pmd_present(*pmd)
pmd_present
pmd_present
*pmd
pmd
pmd
pmd_pfn(*pmd) != pmd_pfn(*new_pmd)
pmd_pfn(*pmd)
pmd_pfn
pmd_pfn
*pmd
pmd
pmd
pmd_pfn(*new_pmd)
pmd_pfn
pmd_pfn
*new_pmd
new_pmd
new_pmd
old_pmd = *pmd;
old_pmd = *pmd
old_pmd
old_pmd
*pmd
pmd
pmd
if (pmd_present(old_pmd)) {		pmd_clear(pmd);		kvm_tlb_flush_vmid_ipa(kvm, addr);	} else {		get_page(virt_to_page(pmd));	}
pmd_present(old_pmd)
pmd_present
pmd_present
old_pmd
old_pmd
{		pmd_clear(pmd);		kvm_tlb_flush_vmid_ipa(kvm, addr);	}
pmd_clear(pmd);
pmd_clear(pmd)
pmd_clear
pmd_clear
pmd
pmd
kvm_tlb_flush_vmid_ipa(kvm, addr);
kvm_tlb_flush_vmid_ipa(kvm, addr)
kvm_tlb_flush_vmid_ipa
kvm_tlb_flush_vmid_ipa
kvm
kvm
addr
addr
{		get_page(virt_to_page(pmd));	}
get_page(virt_to_page(pmd));
get_page(virt_to_page(pmd))
get_page
get_page
virt_to_page(pmd)
virt_to_page
virt_to_page
pmd
pmd
kvm_set_pmd(pmd, *new_pmd);
kvm_set_pmd(pmd, *new_pmd)
kvm_set_pmd
kvm_set_pmd
pmd
pmd
*new_pmd
new_pmd
new_pmd
return 0;
0
-----joern-----
(10,16,0)
(11,31,0)
(15,5,0)
(44,5,0)
(50,15,0)
(8,12,0)
(12,19,0)
(48,47,0)
(12,8,0)
(26,10,0)
(33,23,0)
(0,8,0)
(1,41,0)
(7,15,0)
(38,5,0)
(13,8,0)
(4,11,0)
(17,39,0)
(12,20,0)
(3,17,0)
(10,8,0)
(9,20,0)
(23,33,0)
(22,9,0)
(42,48,0)
(32,28,0)
(8,5,0)
(51,5,0)
(15,8,0)
(24,12,0)
(6,45,0)
(44,0,0)
(33,8,0)
(16,36,0)
(18,33,0)
(15,50,0)
(39,49,0)
(27,9,0)
(23,3,0)
(45,5,0)
(17,3,0)
(46,5,0)
(9,41,0)
(14,47,0)
(40,14,0)
(30,1,0)
(0,44,0)
(3,6,0)
(43,47,0)
(2,39,0)
(50,49,0)
(21,12,0)
(1,8,0)
(11,8,0)
(9,25,0)
(6,3,0)
(16,10,0)
(47,48,0)
(45,6,0)
(6,31,0)
(31,6,0)
(28,5,0)
(48,8,0)
(29,0,0)
(39,17,0)
(37,28,0)
(31,11,0)
(47,5,0)
(34,50,0)
(12,25,0)
(3,23,0)
(35,12,0)
(2,23,1)
(11,8,1)
(10,8,1)
(6,3,1)
(9,27,1)
(22,1,1)
(45,6,1)
(34,7,1)
(6,31,1)
(35,13,1)
(39,49,1)
(7,16,1)
(42,43,1)
(39,2,1)
(12,20,1)
(24,35,1)
(27,22,1)
(31,11,1)
(9,20,1)
(48,42,1)
(12,19,1)
(11,4,1)
(26,14,1)
(47,48,1)
(18,31,1)
(29,8,1)
(9,25,1)
(14,47,1)
(32,28,1)
(8,12,1)
(1,8,1)
(44,0,1)
(7,9,1)
(16,10,1)
(50,34,1)
(10,26,1)
(1,30,1)
(28,37,1)
(17,39,1)
(12,21,1)
(48,8,1)
(37,15,1)
(15,8,1)
(12,25,1)
(43,45,1)
(0,8,1)
(14,40,1)
(0,29,1)
(33,8,1)
(15,50,1)
(3,17,1)
(23,33,1)
(30,14,1)
(21,24,1)
(33,18,1)
(50,49,1)
(40,47,1)
(3,23,1)
(4,44,1)
(16,14,2)
(1,8,2)
(10,8,2)
(9,25,2)
(27,14,2)
(6,3,2)
(12,19,2)
(48,8,2)
(31,11,2)
(30,14,2)
(17,31,2)
(9,20,2)
(33,8,2)
(10,14,2)
(0,8,2)
(1,14,2)
(15,50,2)
(17,39,2)
(47,48,2)
(12,25,2)
(3,23,2)
(44,0,2)
(11,8,2)
(15,8,2)
(3,31,2)
(14,47,2)
(2,31,2)
(23,33,2)
(9,14,2)
(6,31,2)
(16,10,2)
(32,28,2)
(3,17,2)
(22,14,2)
(18,31,2)
(33,31,2)
(39,31,2)
(8,12,2)
(50,49,2)
(23,31,2)
(12,20,2)
(26,14,2)
(39,49,2)
(45,6,2)
-----------------------------------
(0,!pmd)
(1,pmd_clear(pmd)
(2,new_pmd)
(3,pmd_pfn(*pmd)
(4,pmd)
(5,)
(6,pmd_present(*pmd)
(7,pmd)
(8,pmd = stage2_get_pmd(kvm, cache, addr)
(9,kvm_tlb_flush_vmid_ipa(kvm, addr)
(10,virt_to_page(pmd)
(11,*pmd)
(12,stage2_get_pmd(kvm, cache, addr)
(13,pmd)
(14,pmd_present(old_pmd)
(15,kvm_set_pmd(pmd, *new_pmd)
(16,get_page(virt_to_page(pmd)
(17,pmd_pfn(*new_pmd)
(18,pmd)
(19,struct kvm_mmu_memory_cache\n\\n\\t\\t\\t       *cache)
(20,struct kvm *kvm)
(21,addr)
(22,kvm)
(23,pmd_pfn(*pmd)
(24,cache)
(25,phys_addr_t addr)
(26,pmd)
(27,addr)
(28,return 0;)
(29,pmd)
(30,pmd)
(31,pmd_present(*pmd)
(32,RET)
(33,*pmd)
(34,new_pmd)
(35,kvm)
(36,)
(37,0)
(38,old_pmd)
(39,*new_pmd)
(40,old_pmd)
(41,)
(42,pmd)
(43,old_pmd)
(44,VM_BUG_ON(!pmd)
(45,VM_BUG_ON(pmd_present(*pmd)
(46,if (pmd_present(old_pmd)
(47,old_pmd = *pmd)
(48,*pmd)
(49,const pmd_t *new_pmd)
(50,*new_pmd)
(51,pmd)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
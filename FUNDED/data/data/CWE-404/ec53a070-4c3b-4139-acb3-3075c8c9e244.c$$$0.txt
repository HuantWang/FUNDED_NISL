-----label-----
0
-----code-----
static void mlx4_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)
{
	int i;
	int ret = 0;
	struct vm_area_struct *vma;
	struct mlx4_ib_ucontext *context = to_mucontext(ibcontext);
	struct task_struct *owning_process  = NULL;
	struct mm_struct   *owning_mm       = NULL;

	owning_process = get_pid_task(ibcontext->tgid, PIDTYPE_PID);
	if (!owning_process)
		return;

	owning_mm = get_task_mm(owning_process);
	if (!owning_mm) {
		pr_info("no mm, disassociate ucontext is pending task termination\n");
		while (1) {
			/* make sure that task is dead before returning, it may
			 * prevent a rare case of module down in parallel to a
			 * call to mlx4_ib_vma_close.
			 */
			put_task_struct(owning_process);
			msleep(1);
			owning_process = get_pid_task(ibcontext->tgid,
						      PIDTYPE_PID);
			if (!owning_process ||
			    owning_process->state == TASK_DEAD) {
				pr_info("disassociate ucontext done, task was terminated\n");
				/* in case task was dead need to release the task struct */
				if (owning_process)
					put_task_struct(owning_process);
				return;
			}
		}
	}

	/* need to protect from a race on closing the vma as part of
	 * mlx4_ib_vma_close().
	 */
	down_write(&owning_mm->mmap_sem);
	for (i = 0; i < HW_BAR_COUNT; i++) {
		vma = context->hw_bar_info[i].vma;
		if (!vma)
			continue;

		ret = zap_vma_ptes(context->hw_bar_info[i].vma,
				   context->hw_bar_info[i].vma->vm_start,
				   PAGE_SIZE);
		if (ret) {
			pr_err("Error: zap_vma_ptes failed for index=%d, ret=%d\n", i, ret);
			BUG_ON(1);
		}

		context->hw_bar_info[i].vma->vm_flags &=
			~(VM_SHARED | VM_MAYSHARE);
		/* context going to be destroyed, should not access ops any more */
		context->hw_bar_info[i].vma->vm_ops = NULL;
	}

	up_write(&owning_mm->mmap_sem);
	mmput(owning_mm);
	put_task_struct(owning_process);
}
-----children-----
1,2
1,3
1,4
3,4
3,5
5,6
5,7
6,7
8,9
8,10
11,12
11,13
11,14
11,15
11,16
11,17
11,18
11,19
11,20
11,21
11,22
11,23
11,24
11,25
11,26
12,13
13,14
13,15
15,16
17,18
18,19
18,20
20,21
20,22
22,23
24,25
25,26
25,27
26,27
28,29
28,30
31,32
32,33
32,34
33,34
35,36
35,37
35,38
38,39
39,40
39,41
40,41
42,43
44,45
45,46
45,47
46,47
48,49
48,50
48,51
51,52
52,53
54,55
55,56
55,57
56,57
58,59
58,60
58,61
61,62
62,63
64,65
65,66
65,67
66,67
68,69
68,70
68,71
69,70
71,72
71,73
72,73
75,76
77,78
77,79
78,79
79,80
82,83
83,84
83,85
84,85
86,87
86,88
87,88
89,90
91,92
91,93
92,93
93,94
95,96
95,97
96,97
97,98
97,99
98,99
101,102
101,103
103,104
103,105
103,106
103,107
104,105
105,106
105,107
106,107
108,109
110,111
111,112
111,113
112,113
115,116
116,117
116,118
117,118
119,120
119,121
119,122
120,121
122,123
122,124
123,124
126,127
128,129
128,130
129,130
129,131
130,131
131,132
133,134
133,135
134,135
134,136
135,136
138,139
140,141
140,142
140,143
141,142
142,143
142,144
143,144
146,147
146,148
147,148
149,150
150,151
150,152
151,152
153,154
156,157
157,158
157,159
158,159
160,161
161,162
161,163
162,163
165,166
165,167
165,168
165,169
166,167
167,168
167,169
168,169
171,172
171,173
172,173
174,175
176,177
177,178
179,180
179,181
179,182
179,183
179,184
179,185
180,181
181,182
181,183
182,183
184,185
184,186
185,186
185,187
186,187
186,188
187,188
190,191
193,194
193,195
194,195
195,196
198,199
199,200
199,201
200,201
202,203
202,204
202,205
202,206
203,204
205,206
205,207
206,207
206,208
207,208
207,209
208,209
211,212
214,215
214,216
215,216
215,217
216,217
216,218
217,218
217,219
218,219
221,222
225,226
227,228
227,229
228,229
230,231
230,232
231,232
232,233
232,234
232,235
232,236
233,234
236,237
238,239
240,241
241,242
241,243
242,243
245,246
246,247
246,248
247,248
247,249
248,249
248,250
249,250
249,251
250,251
250,252
251,252
254,255
258,259
259,260
260,261
260,262
261,262
263,264
265,266
266,267
266,268
267,268
267,269
268,269
268,270
269,270
269,271
270,271
270,272
271,272
274,275
278,279
280,281
281,282
281,283
282,283
284,285
285,286
285,287
286,287
289,290
290,291
290,292
291,292
293,294
295,296
296,297
296,298
297,298
299,300
-----nextToken-----
2,4,7,9,10,14,16,19,21,23,27,29,30,34,36,37,41,43,47,49,50,53,57,59,60,63,67,70,73,74,76,80,81,85,88,90,94,99,100,102,107,109,113,114,118,121,124,125,127,132,136,137,139,144,145,148,152,154,155,159,163,164,169,170,173,175,178,183,188,189,191,192,196,197,201,204,209,210,212,213,219,220,222,223,224,226,229,234,235,237,239,243,244,252,253,255,256,257,262,264,272,273,275,276,277,279,283,287,288,292,294,298,300
-----computeFrom-----
65,66
65,67
83,84
83,85
116,117
116,118
129,130
129,131
133,134
133,135
167,168
167,169
171,172
171,173
181,182
181,183
199,200
199,201
246,247
246,248
260,261
260,262
266,267
266,268
-----guardedBy-----
136,154
-----guardedByNegation-----
-----lastLexicalUse-----
-----jump-----
-----attribute-----
FunctionDefinition;SimpleDeclSpecifier;FunctionDeclarator;Name;ParameterDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;CompoundStatement;DeclarationStatement;SimpleDeclaration;SimpleDeclSpecifier;Declarator;Name;DeclarationStatement;SimpleDeclaration;SimpleDeclSpecifier;Declarator;Name;EqualsInitializer;LiteralExpression;DeclarationStatement;SimpleDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;DeclarationStatement;SimpleDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;EqualsInitializer;FunctionCallExpression;IdExpression;Name;IdExpression;Name;DeclarationStatement;SimpleDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;EqualsInitializer;IdExpression;Name;DeclarationStatement;SimpleDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;EqualsInitializer;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;FieldReference;IdExpression;Name;Name;IdExpression;Name;IfStatement;UnaryExpression;IdExpression;Name;ReturnStatement;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IfStatement;UnaryExpression;IdExpression;Name;CompoundStatement;ExpressionStatement;FunctionCallExpression;IdExpression;Name;LiteralExpression;WhileStatement;LiteralExpression;CompoundStatement;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;LiteralExpression;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;FieldReference;IdExpression;Name;Name;IdExpression;Name;IfStatement;BinaryExpression;UnaryExpression;IdExpression;Name;BinaryExpression;FieldReference;IdExpression;Name;Name;IdExpression;Name;CompoundStatement;ExpressionStatement;FunctionCallExpression;IdExpression;Name;LiteralExpression;IfStatement;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ReturnStatement;ExpressionStatement;FunctionCallExpression;IdExpression;Name;UnaryExpression;FieldReference;IdExpression;Name;Name;ForStatement;ExpressionStatement;BinaryExpression;IdExpression;Name;LiteralExpression;BinaryExpression;IdExpression;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;CompoundStatement;ExpressionStatement;BinaryExpression;IdExpression;Name;FieldReference;ArraySubscriptExpression;FieldReference;IdExpression;Name;Name;IdExpression;Name;Name;IfStatement;UnaryExpression;IdExpression;Name;ContinueStatement;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;FieldReference;ArraySubscriptExpression;FieldReference;IdExpression;Name;Name;IdExpression;Name;Name;FieldReference;FieldReference;ArraySubscriptExpression;FieldReference;IdExpression;Name;Name;IdExpression;Name;Name;Name;IdExpression;Name;IfStatement;IdExpression;Name;CompoundStatement;ExpressionStatement;FunctionCallExpression;IdExpression;Name;LiteralExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;LiteralExpression;ExpressionStatement;BinaryExpression;FieldReference;FieldReference;ArraySubscriptExpression;FieldReference;IdExpression;Name;Name;IdExpression;Name;Name;Name;UnaryExpression;UnaryExpression;BinaryExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;BinaryExpression;FieldReference;FieldReference;ArraySubscriptExpression;FieldReference;IdExpression;Name;Name;IdExpression;Name;Name;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;UnaryExpression;FieldReference;IdExpression;Name;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;
-----ast_node-----
static void mlx4_ib_disassociate_ucontext(struct ib_ucontext *ibcontext){	int i;	int ret = 0;	struct vm_area_struct *vma;	struct mlx4_ib_ucontext *context = to_mucontext(ibcontext);	struct task_struct *owning_process  = NULL;	struct mm_struct   *owning_mm       = NULL;	owning_process = get_pid_task(ibcontext->tgid, PIDTYPE_PID);	if (!owning_process)		return;	owning_mm = get_task_mm(owning_process);	if (!owning_mm) {		pr_info("no mm, disassociate ucontext is pending task termination\n");		while (1) {			/* make sure that task is dead before returning, it may			 * prevent a rare case of module down in parallel to a			 * call to mlx4_ib_vma_close.			 */			put_task_struct(owning_process);			msleep(1);			owning_process = get_pid_task(ibcontext->tgid,						      PIDTYPE_PID);			if (!owning_process ||			    owning_process->state == TASK_DEAD) {				pr_info("disassociate ucontext done, task was terminated\n");				/* in case task was dead need to release the task struct */				if (owning_process)					put_task_struct(owning_process);				return;			}		}	}	/* need to protect from a race on closing the vma as part of	 * mlx4_ib_vma_close().	 */	down_write(&owning_mm->mmap_sem);	for (i = 0; i < HW_BAR_COUNT; i++) {		vma = context->hw_bar_info[i].vma;		if (!vma)			continue;		ret = zap_vma_ptes(context->hw_bar_info[i].vma,				   context->hw_bar_info[i].vma->vm_start,				   PAGE_SIZE);		if (ret) {			pr_err("Error: zap_vma_ptes failed for index=%d, ret=%d\n", i, ret);			BUG_ON(1);		}		context->hw_bar_info[i].vma->vm_flags &=			~(VM_SHARED | VM_MAYSHARE);		/* context going to be destroyed, should not access ops any more */		context->hw_bar_info[i].vma->vm_ops = NULL;	}	up_write(&owning_mm->mmap_sem);	mmput(owning_mm);	put_task_struct(owning_process);}
static void
mlx4_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)
mlx4_ib_disassociate_ucontext
struct ib_ucontext *ibcontext
struct ib_ucontext
ib_ucontext
*ibcontext
*
ibcontext
{	int i;	int ret = 0;	struct vm_area_struct *vma;	struct mlx4_ib_ucontext *context = to_mucontext(ibcontext);	struct task_struct *owning_process  = NULL;	struct mm_struct   *owning_mm       = NULL;	owning_process = get_pid_task(ibcontext->tgid, PIDTYPE_PID);	if (!owning_process)		return;	owning_mm = get_task_mm(owning_process);	if (!owning_mm) {		pr_info("no mm, disassociate ucontext is pending task termination\n");		while (1) {			/* make sure that task is dead before returning, it may			 * prevent a rare case of module down in parallel to a			 * call to mlx4_ib_vma_close.			 */			put_task_struct(owning_process);			msleep(1);			owning_process = get_pid_task(ibcontext->tgid,						      PIDTYPE_PID);			if (!owning_process ||			    owning_process->state == TASK_DEAD) {				pr_info("disassociate ucontext done, task was terminated\n");				/* in case task was dead need to release the task struct */				if (owning_process)					put_task_struct(owning_process);				return;			}		}	}	/* need to protect from a race on closing the vma as part of	 * mlx4_ib_vma_close().	 */	down_write(&owning_mm->mmap_sem);	for (i = 0; i < HW_BAR_COUNT; i++) {		vma = context->hw_bar_info[i].vma;		if (!vma)			continue;		ret = zap_vma_ptes(context->hw_bar_info[i].vma,				   context->hw_bar_info[i].vma->vm_start,				   PAGE_SIZE);		if (ret) {			pr_err("Error: zap_vma_ptes failed for index=%d, ret=%d\n", i, ret);			BUG_ON(1);		}		context->hw_bar_info[i].vma->vm_flags &=			~(VM_SHARED | VM_MAYSHARE);		/* context going to be destroyed, should not access ops any more */		context->hw_bar_info[i].vma->vm_ops = NULL;	}	up_write(&owning_mm->mmap_sem);	mmput(owning_mm);	put_task_struct(owning_process);}
int i;
int i;
int
i
i
int ret = 0;
int ret = 0;
int
ret = 0
ret
= 0
0
struct vm_area_struct *vma;
struct vm_area_struct *vma;
struct vm_area_struct
vm_area_struct
*vma
*
vma
struct mlx4_ib_ucontext *context = to_mucontext(ibcontext);
struct mlx4_ib_ucontext *context = to_mucontext(ibcontext);
struct mlx4_ib_ucontext
mlx4_ib_ucontext
*context = to_mucontext(ibcontext)
*
context
= to_mucontext(ibcontext)
to_mucontext(ibcontext)
to_mucontext
to_mucontext
ibcontext
ibcontext
struct task_struct *owning_process  = NULL;
struct task_struct *owning_process  = NULL;
struct task_struct
task_struct
*owning_process  = NULL
*
owning_process
= NULL
NULL
NULL
struct mm_struct   *owning_mm       = NULL;
struct mm_struct   *owning_mm       = NULL;
struct mm_struct
mm_struct
*owning_mm       = NULL
*
owning_mm
= NULL
NULL
NULL
owning_process = get_pid_task(ibcontext->tgid, PIDTYPE_PID);
owning_process = get_pid_task(ibcontext->tgid, PIDTYPE_PID)
owning_process
owning_process
get_pid_task(ibcontext->tgid, PIDTYPE_PID)
get_pid_task
get_pid_task
ibcontext->tgid
ibcontext
ibcontext
tgid
PIDTYPE_PID
PIDTYPE_PID
if (!owning_process)		return;
!owning_process
owning_process
owning_process
return;
owning_mm = get_task_mm(owning_process);
owning_mm = get_task_mm(owning_process)
owning_mm
owning_mm
get_task_mm(owning_process)
get_task_mm
get_task_mm
owning_process
owning_process
if (!owning_mm) {		pr_info("no mm, disassociate ucontext is pending task termination\n");		while (1) {			/* make sure that task is dead before returning, it may			 * prevent a rare case of module down in parallel to a			 * call to mlx4_ib_vma_close.			 */			put_task_struct(owning_process);			msleep(1);			owning_process = get_pid_task(ibcontext->tgid,						      PIDTYPE_PID);			if (!owning_process ||			    owning_process->state == TASK_DEAD) {				pr_info("disassociate ucontext done, task was terminated\n");				/* in case task was dead need to release the task struct */				if (owning_process)					put_task_struct(owning_process);				return;			}		}	}
!owning_mm
owning_mm
owning_mm
{		pr_info("no mm, disassociate ucontext is pending task termination\n");		while (1) {			/* make sure that task is dead before returning, it may			 * prevent a rare case of module down in parallel to a			 * call to mlx4_ib_vma_close.			 */			put_task_struct(owning_process);			msleep(1);			owning_process = get_pid_task(ibcontext->tgid,						      PIDTYPE_PID);			if (!owning_process ||			    owning_process->state == TASK_DEAD) {				pr_info("disassociate ucontext done, task was terminated\n");				/* in case task was dead need to release the task struct */				if (owning_process)					put_task_struct(owning_process);				return;			}		}	}
pr_info("no mm, disassociate ucontext is pending task termination\n");
pr_info("no mm, disassociate ucontext is pending task termination\n")
pr_info
pr_info
"no mm, disassociate ucontext is pending task termination\n"
while (1) {			/* make sure that task is dead before returning, it may			 * prevent a rare case of module down in parallel to a			 * call to mlx4_ib_vma_close.			 */			put_task_struct(owning_process);			msleep(1);			owning_process = get_pid_task(ibcontext->tgid,						      PIDTYPE_PID);			if (!owning_process ||			    owning_process->state == TASK_DEAD) {				pr_info("disassociate ucontext done, task was terminated\n");				/* in case task was dead need to release the task struct */				if (owning_process)					put_task_struct(owning_process);				return;			}		}
1
{			/* make sure that task is dead before returning, it may			 * prevent a rare case of module down in parallel to a			 * call to mlx4_ib_vma_close.			 */			put_task_struct(owning_process);			msleep(1);			owning_process = get_pid_task(ibcontext->tgid,						      PIDTYPE_PID);			if (!owning_process ||			    owning_process->state == TASK_DEAD) {				pr_info("disassociate ucontext done, task was terminated\n");				/* in case task was dead need to release the task struct */				if (owning_process)					put_task_struct(owning_process);				return;			}		}
put_task_struct(owning_process);
put_task_struct(owning_process)
put_task_struct
put_task_struct
owning_process
owning_process
msleep(1);
msleep(1)
msleep
msleep
1
owning_process = get_pid_task(ibcontext->tgid,						      PIDTYPE_PID);
owning_process = get_pid_task(ibcontext->tgid,						      PIDTYPE_PID)
owning_process
owning_process
get_pid_task(ibcontext->tgid,						      PIDTYPE_PID)
get_pid_task
get_pid_task
ibcontext->tgid
ibcontext
ibcontext
tgid
PIDTYPE_PID
PIDTYPE_PID
if (!owning_process ||			    owning_process->state == TASK_DEAD) {				pr_info("disassociate ucontext done, task was terminated\n");				/* in case task was dead need to release the task struct */				if (owning_process)					put_task_struct(owning_process);				return;			}
!owning_process ||			    owning_process->state == TASK_DEAD
!owning_process
owning_process
owning_process
owning_process->state == TASK_DEAD
owning_process->state
owning_process
owning_process
state
TASK_DEAD
TASK_DEAD
{				pr_info("disassociate ucontext done, task was terminated\n");				/* in case task was dead need to release the task struct */				if (owning_process)					put_task_struct(owning_process);				return;			}
pr_info("disassociate ucontext done, task was terminated\n");
pr_info("disassociate ucontext done, task was terminated\n")
pr_info
pr_info
"disassociate ucontext done, task was terminated\n"
if (owning_process)					put_task_struct(owning_process);
owning_process
owning_process
put_task_struct(owning_process);
put_task_struct(owning_process)
put_task_struct
put_task_struct
owning_process
owning_process
return;
down_write(&owning_mm->mmap_sem);
down_write(&owning_mm->mmap_sem)
down_write
down_write
&owning_mm->mmap_sem
owning_mm->mmap_sem
owning_mm
owning_mm
mmap_sem
for (i = 0; i < HW_BAR_COUNT; i++) {		vma = context->hw_bar_info[i].vma;		if (!vma)			continue;		ret = zap_vma_ptes(context->hw_bar_info[i].vma,				   context->hw_bar_info[i].vma->vm_start,				   PAGE_SIZE);		if (ret) {			pr_err("Error: zap_vma_ptes failed for index=%d, ret=%d\n", i, ret);			BUG_ON(1);		}		context->hw_bar_info[i].vma->vm_flags &=			~(VM_SHARED | VM_MAYSHARE);		/* context going to be destroyed, should not access ops any more */		context->hw_bar_info[i].vma->vm_ops = NULL;	}
i = 0;
i = 0
i
i
0
i < HW_BAR_COUNT
i
i
HW_BAR_COUNT
HW_BAR_COUNT
i++
i
i
{		vma = context->hw_bar_info[i].vma;		if (!vma)			continue;		ret = zap_vma_ptes(context->hw_bar_info[i].vma,				   context->hw_bar_info[i].vma->vm_start,				   PAGE_SIZE);		if (ret) {			pr_err("Error: zap_vma_ptes failed for index=%d, ret=%d\n", i, ret);			BUG_ON(1);		}		context->hw_bar_info[i].vma->vm_flags &=			~(VM_SHARED | VM_MAYSHARE);		/* context going to be destroyed, should not access ops any more */		context->hw_bar_info[i].vma->vm_ops = NULL;	}
vma = context->hw_bar_info[i].vma;
vma = context->hw_bar_info[i].vma
vma
vma
context->hw_bar_info[i].vma
context->hw_bar_info[i]
context->hw_bar_info
context
context
hw_bar_info
i
i
vma
if (!vma)			continue;
!vma
vma
vma
continue;
ret = zap_vma_ptes(context->hw_bar_info[i].vma,				   context->hw_bar_info[i].vma->vm_start,				   PAGE_SIZE);
ret = zap_vma_ptes(context->hw_bar_info[i].vma,				   context->hw_bar_info[i].vma->vm_start,				   PAGE_SIZE)
ret
ret
zap_vma_ptes(context->hw_bar_info[i].vma,				   context->hw_bar_info[i].vma->vm_start,				   PAGE_SIZE)
zap_vma_ptes
zap_vma_ptes
context->hw_bar_info[i].vma
context->hw_bar_info[i]
context->hw_bar_info
context
context
hw_bar_info
i
i
vma
context->hw_bar_info[i].vma->vm_start
context->hw_bar_info[i].vma
context->hw_bar_info[i]
context->hw_bar_info
context
context
hw_bar_info
i
i
vma
vm_start
PAGE_SIZE
PAGE_SIZE
if (ret) {			pr_err("Error: zap_vma_ptes failed for index=%d, ret=%d\n", i, ret);			BUG_ON(1);		}
ret
ret
{			pr_err("Error: zap_vma_ptes failed for index=%d, ret=%d\n", i, ret);			BUG_ON(1);		}
pr_err("Error: zap_vma_ptes failed for index=%d, ret=%d\n", i, ret);
pr_err("Error: zap_vma_ptes failed for index=%d, ret=%d\n", i, ret)
pr_err
pr_err
"Error: zap_vma_ptes failed for index=%d, ret=%d\n"
i
i
ret
ret
BUG_ON(1);
BUG_ON(1)
BUG_ON
BUG_ON
1
context->hw_bar_info[i].vma->vm_flags &=			~(VM_SHARED | VM_MAYSHARE);
context->hw_bar_info[i].vma->vm_flags &=			~(VM_SHARED | VM_MAYSHARE)
context->hw_bar_info[i].vma->vm_flags
context->hw_bar_info[i].vma
context->hw_bar_info[i]
context->hw_bar_info
context
context
hw_bar_info
i
i
vma
vm_flags
~(VM_SHARED | VM_MAYSHARE)
(VM_SHARED | VM_MAYSHARE)
VM_SHARED | VM_MAYSHARE
VM_SHARED
VM_SHARED
VM_MAYSHARE
VM_MAYSHARE
context->hw_bar_info[i].vma->vm_ops = NULL;
context->hw_bar_info[i].vma->vm_ops = NULL
context->hw_bar_info[i].vma->vm_ops
context->hw_bar_info[i].vma
context->hw_bar_info[i]
context->hw_bar_info
context
context
hw_bar_info
i
i
vma
vm_ops
NULL
NULL
up_write(&owning_mm->mmap_sem);
up_write(&owning_mm->mmap_sem)
up_write
up_write
&owning_mm->mmap_sem
owning_mm->mmap_sem
owning_mm
owning_mm
mmap_sem
mmput(owning_mm);
mmput(owning_mm)
mmput
mmput
owning_mm
owning_mm
put_task_struct(owning_process);
put_task_struct(owning_process)
put_task_struct
put_task_struct
owning_process
owning_process
-----joern-----
(31,30,0)
(85,151,0)
(97,83,0)
(135,58,0)
(160,60,0)
(34,82,0)
(117,60,0)
(54,134,0)
(73,160,0)
(11,30,0)
(56,62,0)
(45,30,0)
(18,38,0)
(112,160,0)
(38,30,0)
(83,97,0)
(42,27,0)
(6,9,0)
(127,12,0)
(101,100,0)
(88,41,0)
(18,143,0)
(162,45,0)
(4,97,0)
(27,164,0)
(80,92,0)
(16,30,0)
(161,118,0)
(92,132,0)
(19,104,0)
(22,41,0)
(71,12,0)
(156,164,0)
(82,152,0)
(66,23,0)
(37,22,0)
(38,150,0)
(126,159,0)
(3,71,0)
(20,100,0)
(8,36,0)
(115,14,0)
(21,18,0)
(39,30,0)
(150,147,0)
(12,127,0)
(7,23,0)
(121,104,0)
(161,83,0)
(138,30,0)
(3,108,0)
(128,45,0)
(82,38,0)
(56,104,0)
(134,38,0)
(17,31,0)
(50,23,0)
(4,152,0)
(143,23,0)
(19,30,0)
(58,160,0)
(49,23,0)
(110,49,0)
(155,129,0)
(14,160,0)
(29,162,0)
(27,42,0)
(5,56,0)
(118,161,0)
(32,107,0)
(128,160,0)
(154,108,0)
(159,126,0)
(136,126,0)
(113,48,0)
(92,147,0)
(99,81,0)
(63,134,0)
(62,56,0)
(25,30,0)
(111,12,0)
(61,146,0)
(74,82,0)
(151,55,0)
(3,23,0)
(123,127,0)
(152,23,0)
(162,104,0)
(159,160,0)
(85,44,0)
(132,160,0)
(86,19,0)
(45,122,0)
(70,61,0)
(48,122,0)
(143,42,0)
(84,109,0)
(76,117,0)
(79,146,0)
(143,18,0)
(102,36,0)
(139,56,0)
(140,118,0)
(127,37,0)
(109,55,0)
(64,73,0)
(100,30,0)
(153,58,0)
(96,152,0)
(52,9,0)
(83,146,0)
(65,37,0)
(128,60,0)
(22,37,0)
(7,134,0)
(119,30,0)
(150,38,0)
(28,42,0)
(77,146,0)
(158,131,0)
(137,43,0)
(1,22,0)
(122,48,0)
(42,143,0)
(68,38,0)
(40,45,0)
(141,7,0)
(124,23,0)
(36,30,0)
(46,104,0)
(41,22,0)
(9,7,0)
(41,38,0)
(160,132,0)
(13,18,0)
(14,45,0)
(104,30,0)
(58,136,0)
(37,127,0)
(2,128,0)
(152,4,0)
(78,136,0)
(57,17,0)
(15,4,0)
(48,147,0)
(17,57,0)
(10,125,0)
(57,104,0)
(51,71,0)
(126,136,0)
(152,82,0)
(125,45,0)
(98,97,0)
(106,159,0)
(130,49,0)
(164,146,0)
(93,48,0)
(43,59,0)
(133,66,0)
(14,30,0)
(95,52,0)
(108,3,0)
(75,57,0)
(108,38,0)
(145,121,0)
(148,41,0)
(132,92,0)
(62,47,0)
(24,132,0)
(103,124,0)
(157,108,0)
(0,150,0)
(7,9,0)
(114,55,0)
(105,3,0)
(144,122,0)
(47,30,0)
(90,57,0)
(149,118,0)
(94,27,0)
(122,45,0)
(127,61,0)
(107,129,0)
(26,30,0)
(163,23,0)
(124,59,0)
(53,124,0)
(9,52,0)
(134,7,0)
(87,143,0)
(61,127,0)
(142,60,0)
(31,17,0)
(69,30,0)
(22,23,0)
(136,58,0)
(33,92,0)
(131,30,0)
(116,30,0)
(104,162,0)
(89,131,0)
(124,61,0)
(71,3,0)
(81,52,0)
(52,146,0)
(97,4,0)
(83,161,0)
(12,71,0)
(47,62,0)
(91,124,0)
(73,160,1)
(120,81,1)
(83,97,1)
(92,80,1)
(110,66,1)
(102,38,1)
(35,107,1)
(18,38,1)
(70,81,1)
(14,160,1)
(3,23,1)
(74,67,1)
(75,49,1)
(91,103,1)
(125,10,1)
(24,92,1)
(15,152,1)
(27,94,1)
(128,2,1)
(18,13,1)
(88,70,1)
(82,38,1)
(4,15,1)
(36,8,1)
(99,52,1)
(9,6,1)
(31,17,1)
(151,72,1)
(164,156,1)
(93,113,1)
(121,145,1)
(133,164,1)
(61,127,1)
(42,28,1)
(132,92,1)
(38,150,1)
(84,126,1)
(47,62,1)
(118,140,1)
(64,72,1)
(87,18,1)
(49,130,1)
(3,105,1)
(42,143,1)
(92,147,1)
(109,84,1)
(123,12,1)
(53,91,1)
(134,38,1)
(124,23,1)
(128,160,1)
(108,38,1)
(8,102,1)
(86,31,1)
(143,18,1)
(127,123,1)
(22,23,1)
(7,141,1)
(41,148,1)
(81,99,1)
(108,154,1)
(7,23,1)
(12,111,1)
(67,61,1)
(127,37,1)
(136,78,1)
(140,149,1)
(152,82,1)
(149,97,1)
(101,20,1)
(111,71,1)
(23,163,1)
(51,3,1)
(41,38,1)
(14,115,1)
(152,23,1)
(49,23,1)
(162,29,1)
(150,147,1)
(157,37,1)
(160,132,1)
(34,74,1)
(57,90,1)
(13,21,1)
(2,35,1)
(145,104,1)
(78,58,1)
(17,57,1)
(63,54,1)
(52,9,1)
(5,139,1)
(20,36,1)
(40,100,1)
(121,104,1)
(136,58,1)
(56,5,1)
(122,144,1)
(27,42,1)
(125,45,1)
(48,93,1)
(128,45,1)
(124,61,1)
(85,14,1)
(80,33,1)
(72,109,1)
(104,162,1)
(7,134,1)
(117,76,1)
(28,143,1)
(135,159,1)
(100,101,1)
(137,124,1)
(144,48,1)
(112,117,1)
(159,160,1)
(45,122,1)
(66,23,1)
(126,159,1)
(81,52,1)
(19,86,1)
(154,157,1)
(122,48,1)
(73,64,1)
(141,134,1)
(1,41,1)
(74,43,1)
(65,22,1)
(48,147,1)
(126,136,1)
(110,23,1)
(153,135,1)
(68,131,1)
(156,27,1)
(107,32,1)
(46,125,1)
(159,106,1)
(50,47,1)
(76,128,1)
(57,104,1)
(58,160,1)
(71,51,1)
(132,24,1)
(139,121,1)
(66,133,1)
(115,19,1)
(4,152,1)
(130,110,1)
(143,87,1)
(6,7,1)
(54,95,1)
(85,44,1)
(83,161,1)
(32,121,1)
(98,4,1)
(29,46,1)
(151,73,1)
(33,112,1)
(0,68,1)
(82,34,1)
(44,125,1)
(10,45,1)
(106,160,1)
(22,41,1)
(90,75,1)
(71,3,1)
(162,45,1)
(58,153,1)
(133,120,1)
(85,151,1)
(103,67,1)
(95,49,1)
(9,7,1)
(143,23,1)
(37,65,1)
(161,118,1)
(62,56,1)
(14,45,1)
(43,137,1)
(148,88,1)
(124,53,1)
(139,35,1)
(158,89,1)
(152,96,1)
(127,12,1)
(94,42,1)
(150,0,1)
(105,108,1)
(37,22,1)
(131,158,1)
(21,83,1)
(134,63,1)
(12,71,1)
(22,1,1)
(97,4,1)
(35,126,1)
(163,50,1)
(97,98,1)
(56,104,1)
(113,40,1)
(3,108,1)
(96,82,1)
(19,104,1)
(152,23,2)
(52,9,2)
(110,121,2)
(115,35,2)
(57,35,2)
(64,72,2)
(162,45,2)
(74,81,2)
(35,121,2)
(108,38,2)
(22,81,2)
(86,121,2)
(80,35,2)
(12,71,2)
(95,49,2)
(124,61,2)
(54,49,2)
(121,104,2)
(82,81,2)
(51,81,2)
(4,152,2)
(32,121,2)
(46,125,2)
(3,81,2)
(163,121,2)
(124,67,2)
(156,81,2)
(121,125,2)
(24,35,2)
(127,81,2)
(31,121,2)
(133,49,2)
(72,126,2)
(103,67,2)
(85,151,2)
(14,45,2)
(66,23,2)
(126,35,2)
(56,104,2)
(152,81,2)
(110,49,2)
(112,35,2)
(62,35,2)
(105,81,2)
(161,118,2)
(128,35,2)
(48,147,2)
(50,121,2)
(104,162,2)
(143,23,2)
(70,81,2)
(83,97,2)
(150,147,2)
(125,45,2)
(65,81,2)
(33,35,2)
(160,35,2)
(111,81,2)
(145,125,2)
(134,38,2)
(127,37,2)
(118,81,2)
(17,35,2)
(38,150,2)
(42,143,2)
(130,49,2)
(162,125,2)
(134,49,2)
(154,81,2)
(57,121,2)
(149,81,2)
(14,121,2)
(90,35,2)
(75,121,2)
(143,18,2)
(97,81,2)
(151,126,2)
(42,81,2)
(1,81,2)
(87,81,2)
(58,160,2)
(76,35,2)
(6,49,2)
(117,35,2)
(123,81,2)
(7,23,2)
(71,3,2)
(106,35,2)
(78,159,2)
(63,49,2)
(99,49,2)
(19,121,2)
(61,127,2)
(3,108,2)
(43,67,2)
(49,23,2)
(107,121,2)
(73,160,2)
(14,160,2)
(3,23,2)
(130,121,2)
(49,121,2)
(49,35,2)
(83,81,2)
(2,35,2)
(159,35,2)
(139,35,2)
(52,49,2)
(81,52,2)
(57,104,2)
(85,44,2)
(81,49,2)
(18,81,2)
(132,35,2)
(94,81,2)
(15,81,2)
(58,159,2)
(143,81,2)
(140,81,2)
(82,38,2)
(84,126,2)
(122,48,2)
(12,81,2)
(47,62,2)
(110,35,2)
(152,82,2)
(14,35,2)
(97,4,2)
(141,49,2)
(41,38,2)
(137,67,2)
(5,35,2)
(160,132,2)
(73,72,2)
(62,56,2)
(18,38,2)
(56,35,2)
(132,92,2)
(4,81,2)
(148,81,2)
(49,49,2)
(34,81,2)
(62,121,2)
(88,81,2)
(31,35,2)
(130,35,2)
(45,122,2)
(67,81,2)
(157,81,2)
(27,42,2)
(161,81,2)
(19,35,2)
(91,67,2)
(5,121,2)
(41,81,2)
(23,35,2)
(126,159,2)
(109,126,2)
(126,136,2)
(61,81,2)
(21,81,2)
(92,35,2)
(7,134,2)
(153,159,2)
(27,81,2)
(108,81,2)
(66,49,2)
(96,81,2)
(139,121,2)
(31,17,2)
(22,41,2)
(98,81,2)
(22,23,2)
(163,35,2)
(71,81,2)
(47,35,2)
(136,159,2)
(37,22,2)
(47,121,2)
(120,81,2)
(13,81,2)
(92,147,2)
(50,35,2)
(44,125,2)
(115,121,2)
(23,121,2)
(17,57,2)
(29,125,2)
(28,81,2)
(56,121,2)
(75,35,2)
(9,49,2)
(124,23,2)
(128,160,2)
(136,58,2)
(7,49,2)
(19,104,2)
(104,125,2)
(53,67,2)
(164,81,2)
(83,161,2)
(86,35,2)
(127,12,2)
(9,7,2)
(35,126,2)
(159,160,2)
(128,45,2)
(135,159,2)
(17,121,2)
(90,121,2)
(37,81,2)
-----------------------------------
(0,ibcontext)
(1,i)
(2,owning_process)
(3,context->hw_bar_info[i])
(4,context->hw_bar_info[i].vma)
(5,mmap_sem)
(6,vma)
(7,context->hw_bar_info[i])
(8,NULL)
(9,context->hw_bar_info[i].vma)
(10,owning_process)
(11,vma)
(12,context->hw_bar_info[i].vma->vm_start)
(13,hw_bar_info)
(14,put_task_struct(owning_process)
(15,vma)
(16,i)
(17,&owning_mm->mmap_sem)
(18,context->hw_bar_info)
(19,mmput(owning_mm)
(20,owning_mm)
(21,context)
(22,context->hw_bar_info[i])
(23,i = 0)
(24,PIDTYPE_PID)
(25,for (i = 0; i < HW_BAR_COUNT; i++)
(26,owning_process)
(27,context->hw_bar_info[i].vma->vm_ops)
(28,vma)
(29,owning_process)
(30,)
(31,up_write(&owning_mm->mmap_sem)
(32,"no mm, disassociate ucontext is pending task termination\\n")
(33,ibcontext)
(34,hw_bar_info)
(35,1)
(36,*owning_process  = NULL)
(37,context->hw_bar_info[i].vma)
(38,*context = to_mucontext(ibcontext)
(39,context)
(40,owning_process)
(41,context->hw_bar_info)
(42,context->hw_bar_info[i].vma)
(43,BUG_ON(1)
(44,return;)
(45,owning_process = get_pid_task(ibcontext->tgid, PIDTYPE_PID)
(46,owning_mm)
(47,down_write(&owning_mm->mmap_sem)
(48,ibcontext->tgid)
(49,i < HW_BAR_COUNT)
(50,i)
(51,vma)
(52,vma = context->hw_bar_info[i].vma)
(53,ret)
(54,context)
(55,)
(56,owning_mm->mmap_sem)
(57,owning_mm->mmap_sem)
(58,owning_process->state)
(59,)
(60,)
(61,ret = zap_vma_ptes(context->hw_bar_info[i].vma,\n\\n\\t\\t\\t\\t   context->hw_bar_info[i].vma->vm_start,\n\\n\\t\\t\\t\\t   PAGE_SIZE)
(62,&owning_mm->mmap_sem)
(63,hw_bar_info)
(64,owning_process)
(65,vma)
(66,i++)
(67,ret)
(68,context)
(69,if (!owning_process)
(70,ret)
(71,context->hw_bar_info[i].vma)
(72,owning_process)
(73,put_task_struct(owning_process)
(74,context)
(75,owning_mm)
(76,1)
(77,if (ret)
(78,TASK_DEAD)
(79,if (!vma)
(80,tgid)
(81,!vma)
(82,context->hw_bar_info)
(83,context->hw_bar_info[i].vma->vm_flags &=\n\\n\\t\\t\\t~(VM_SHARED | VM_MAYSHARE)
(84,"disassociate ucontext done, task was terminated\\n")
(85,RET)
(86,owning_mm)
(87,i)
(88,context)
(89,ret)
(90,mmap_sem)
(91,i)
(92,ibcontext->tgid)
(93,tgid)
(94,vm_ops)
(95,vma)
(96,i)
(97,context->hw_bar_info[i].vma->vm_flags)
(98,vm_flags)
(99,vma)
(100,*owning_mm       = NULL)
(101,NULL)
(102,owning_process)
(103,"Error: zap_vma_ptes failed for index=%d, ret=%d\\n")
(104,owning_mm = get_task_mm(owning_process)
(105,i)
(106,owning_process)
(107,pr_info("no mm, disassociate ucontext is pending task termination\\n")
(108,context->hw_bar_info)
(109,pr_info("disassociate ucontext done, task was terminated\\n")
(110,i)
(111,vm_start)
(112,owning_process)
(113,ibcontext)
(114,if (owning_process)
(115,owning_process)
(116,owning_mm)
(117,msleep(1)
(118,VM_SHARED | VM_MAYSHARE)
(119,if (!owning_mm)
(120,continue;)
(121,!owning_mm)
(122,get_pid_task(ibcontext->tgid, PIDTYPE_PID)
(123,PAGE_SIZE)
(124,pr_err("Error: zap_vma_ptes failed for index=%d, ret=%d\\n", i, ret)
(125,!owning_process)
(126,!owning_process ||\n\\n\\t\\t\\t    owning_process->state == TASK_DEAD)
(127,zap_vma_ptes(context->hw_bar_info[i].vma,\n\\n\\t\\t\\t\\t   context->hw_bar_info[i].vma->vm_start,\n\\n\\t\\t\\t\\t   PAGE_SIZE)
(128,put_task_struct(owning_process)
(129,)
(130,HW_BAR_COUNT)
(131,ret = 0)
(132,get_pid_task(ibcontext->tgid,\n\\n\\t\\t\\t\\t\\t\\t      PIDTYPE_PID)
(133,i)
(134,context->hw_bar_info)
(135,owning_process)
(136,owning_process->state == TASK_DEAD)
(137,1)
(138,ret)
(139,owning_mm)
(140,VM_MAYSHARE)
(141,i)
(142,if (!owning_process ||\n\\n\\t\\t\\t    owning_process->state == TASK_DEAD)
(143,context->hw_bar_info[i])
(144,PIDTYPE_PID)
(145,owning_mm)
(146,)
(147,struct ib_ucontext *ibcontext)
(148,hw_bar_info)
(149,VM_SHARED)
(150,to_mucontext(ibcontext)
(151,return;)
(152,context->hw_bar_info[i])
(153,state)
(154,hw_bar_info)
(155,while (1)
(156,NULL)
(157,context)
(158,0)
(159,!owning_process)
(160,owning_process = get_pid_task(ibcontext->tgid,\n\\n\\t\\t\\t\\t\\t\\t      PIDTYPE_PID)
(161,~(VM_SHARED | VM_MAYSHARE)
(162,get_task_mm(owning_process)
(163,0)
(164,context->hw_bar_info[i].vma->vm_ops = NULL)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
-----label-----
1
-----code-----
static int stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache
			       *cache, phys_addr_t addr, const pmd_t *new_pmd)
{
	pmd_t *pmd, old_pmd;

	pmd = stage2_get_pmd(kvm, cache, addr);
	VM_BUG_ON(!pmd);

	old_pmd = *pmd;
	if (pmd_present(old_pmd)) {
		/*
		 * Multiple vcpus faulting on the same PMD entry, can
		 * lead to them sequentially updating the PMD with the
		 * same value. Following the break-before-make
		 * (pmd_clear() followed by tlb_flush()) process can
		 * hinder forward progress due to refaults generated
		 * on missing translations.
		 *
		 * Skip updating the page table if the entry is
		 * unchanged.
		 */
		if (pmd_val(old_pmd) == pmd_val(*new_pmd))
			return 0;

		/*
		 * Mapping in huge pages should only happen through a
		 * fault.  If a page is merged into a transparent huge
		 * page, the individual subpages of that huge page
		 * should be unmapped through MMU notifiers before we
		 * get here.
		 *
		 * Merging of CompoundPages is not supported; they
		 * should become splitting first, unmapped, merged,
		 * and mapped back in on-demand.
		 */
		VM_BUG_ON(pmd_pfn(old_pmd) != pmd_pfn(*new_pmd));

		pmd_clear(pmd);
		kvm_tlb_flush_vmid_ipa(kvm, addr);
	} else {
		get_page(virt_to_page(pmd));
	}

	kvm_set_pmd(pmd, *new_pmd);
	return 0;
}
-----children-----
1,2
1,3
1,4
3,4
3,5
3,6
3,7
3,8
5,6
5,7
6,7
8,9
8,10
11,12
11,13
12,13
14,15
14,16
17,18
17,19
18,19
20,21
22,23
22,24
23,24
25,26
25,27
28,29
28,30
28,31
28,32
28,33
28,34
28,35
29,30
30,31
30,32
30,33
31,32
33,34
33,35
36,37
38,39
39,40
39,41
40,41
42,43
42,44
42,45
42,46
43,44
45,46
47,48
49,50
51,52
52,53
52,54
53,54
55,56
56,57
58,59
59,60
59,61
60,61
62,63
63,64
65,66
65,67
65,68
66,67
66,68
67,68
69,70
71,72
71,73
71,74
71,75
72,73
72,74
73,74
73,75
74,75
74,76
75,76
77,78
79,80
79,81
80,81
82,83
83,84
85,86
87,88
88,89
88,90
89,90
91,92
91,93
92,93
92,94
93,94
95,96
97,98
97,99
98,99
100,101
101,102
103,104
104,105
104,106
105,106
107,108
109,110
110,111
110,112
110,113
111,112
113,114
115,116
117,118
118,119
119,120
119,121
120,121
122,123
122,124
123,124
125,126
127,128
128,129
128,130
128,131
129,130
131,132
133,134
134,135
136,137
-----nextToken-----
2,4,7,9,10,13,15,16,19,21,24,26,27,32,34,35,37,41,44,46,48,50,54,57,61,64,68,70,76,78,81,84,86,90,94,96,99,102,106,108,112,114,116,121,124,126,130,132,135,137
-----computeFrom-----
39,40
39,41
59,60
59,61
73,74
73,75
91,92
91,93
-----guardedBy-----
-----guardedByNegation-----
-----lastLexicalUse-----
-----jump-----
-----attribute-----
FunctionDefinition;SimpleDeclSpecifier;FunctionDeclarator;Name;ParameterDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;ParameterDeclaration;ElaboratedTypeSpecifier;Name;Declarator;Pointer;Name;ParameterDeclaration;NamedTypeSpecifier;Name;Declarator;Name;ParameterDeclaration;NamedTypeSpecifier;Name;Declarator;Pointer;Name;CompoundStatement;DeclarationStatement;SimpleDeclaration;NamedTypeSpecifier;Name;Declarator;Pointer;Name;Declarator;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;ExpressionStatement;BinaryExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;IfStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;CompoundStatement;IfStatement;BinaryExpression;FunctionCallExpression;IdExpression;Name;IdExpression;Name;FunctionCallExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;ReturnStatement;LiteralExpression;ExpressionStatement;FunctionCallExpression;IdExpression;Name;BinaryExpression;FunctionCallExpression;IdExpression;Name;IdExpression;Name;FunctionCallExpression;IdExpression;Name;UnaryExpression;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;IdExpression;Name;CompoundStatement;ExpressionStatement;FunctionCallExpression;IdExpression;Name;FunctionCallExpression;IdExpression;Name;IdExpression;Name;ExpressionStatement;FunctionCallExpression;IdExpression;Name;IdExpression;Name;UnaryExpression;IdExpression;Name;ReturnStatement;LiteralExpression;
-----ast_node-----
static int stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache			       *cache, phys_addr_t addr, const pmd_t *new_pmd){	pmd_t *pmd, old_pmd;	pmd = stage2_get_pmd(kvm, cache, addr);	VM_BUG_ON(!pmd);	old_pmd = *pmd;	if (pmd_present(old_pmd)) {		/*		 * Multiple vcpus faulting on the same PMD entry, can		 * lead to them sequentially updating the PMD with the		 * same value. Following the break-before-make		 * (pmd_clear() followed by tlb_flush()) process can		 * hinder forward progress due to refaults generated		 * on missing translations.		 *		 * Skip updating the page table if the entry is		 * unchanged.		 */		if (pmd_val(old_pmd) == pmd_val(*new_pmd))			return 0;		/*		 * Mapping in huge pages should only happen through a		 * fault.  If a page is merged into a transparent huge		 * page, the individual subpages of that huge page		 * should be unmapped through MMU notifiers before we		 * get here.		 *		 * Merging of CompoundPages is not supported; they		 * should become splitting first, unmapped, merged,		 * and mapped back in on-demand.		 */		VM_BUG_ON(pmd_pfn(old_pmd) != pmd_pfn(*new_pmd));		pmd_clear(pmd);		kvm_tlb_flush_vmid_ipa(kvm, addr);	} else {		get_page(virt_to_page(pmd));	}	kvm_set_pmd(pmd, *new_pmd);	return 0;}
static int
stage2_set_pmd_huge(struct kvm *kvm, struct kvm_mmu_memory_cache			       *cache, phys_addr_t addr, const pmd_t *new_pmd)
stage2_set_pmd_huge
struct kvm *kvm
struct kvm
kvm
*kvm
*
kvm
struct kvm_mmu_memory_cache			       *cache
struct kvm_mmu_memory_cache
kvm_mmu_memory_cache
*cache
*
cache
phys_addr_t addr
phys_addr_t
phys_addr_t
addr
addr
const pmd_t *new_pmd
const pmd_t
pmd_t
*new_pmd
*
new_pmd
{	pmd_t *pmd, old_pmd;	pmd = stage2_get_pmd(kvm, cache, addr);	VM_BUG_ON(!pmd);	old_pmd = *pmd;	if (pmd_present(old_pmd)) {		/*		 * Multiple vcpus faulting on the same PMD entry, can		 * lead to them sequentially updating the PMD with the		 * same value. Following the break-before-make		 * (pmd_clear() followed by tlb_flush()) process can		 * hinder forward progress due to refaults generated		 * on missing translations.		 *		 * Skip updating the page table if the entry is		 * unchanged.		 */		if (pmd_val(old_pmd) == pmd_val(*new_pmd))			return 0;		/*		 * Mapping in huge pages should only happen through a		 * fault.  If a page is merged into a transparent huge		 * page, the individual subpages of that huge page		 * should be unmapped through MMU notifiers before we		 * get here.		 *		 * Merging of CompoundPages is not supported; they		 * should become splitting first, unmapped, merged,		 * and mapped back in on-demand.		 */		VM_BUG_ON(pmd_pfn(old_pmd) != pmd_pfn(*new_pmd));		pmd_clear(pmd);		kvm_tlb_flush_vmid_ipa(kvm, addr);	} else {		get_page(virt_to_page(pmd));	}	kvm_set_pmd(pmd, *new_pmd);	return 0;}
pmd_t *pmd, old_pmd;
pmd_t *pmd, old_pmd;
pmd_t
pmd_t
*pmd
*
pmd
old_pmd
old_pmd
pmd = stage2_get_pmd(kvm, cache, addr);
pmd = stage2_get_pmd(kvm, cache, addr)
pmd
pmd
stage2_get_pmd(kvm, cache, addr)
stage2_get_pmd
stage2_get_pmd
kvm
kvm
cache
cache
addr
addr
VM_BUG_ON(!pmd);
VM_BUG_ON(!pmd)
VM_BUG_ON
VM_BUG_ON
!pmd
pmd
pmd
old_pmd = *pmd;
old_pmd = *pmd
old_pmd
old_pmd
*pmd
pmd
pmd
if (pmd_present(old_pmd)) {		/*		 * Multiple vcpus faulting on the same PMD entry, can		 * lead to them sequentially updating the PMD with the		 * same value. Following the break-before-make		 * (pmd_clear() followed by tlb_flush()) process can		 * hinder forward progress due to refaults generated		 * on missing translations.		 *		 * Skip updating the page table if the entry is		 * unchanged.		 */		if (pmd_val(old_pmd) == pmd_val(*new_pmd))			return 0;		/*		 * Mapping in huge pages should only happen through a		 * fault.  If a page is merged into a transparent huge		 * page, the individual subpages of that huge page		 * should be unmapped through MMU notifiers before we		 * get here.		 *		 * Merging of CompoundPages is not supported; they		 * should become splitting first, unmapped, merged,		 * and mapped back in on-demand.		 */		VM_BUG_ON(pmd_pfn(old_pmd) != pmd_pfn(*new_pmd));		pmd_clear(pmd);		kvm_tlb_flush_vmid_ipa(kvm, addr);	} else {		get_page(virt_to_page(pmd));	}
pmd_present(old_pmd)
pmd_present
pmd_present
old_pmd
old_pmd
{		/*		 * Multiple vcpus faulting on the same PMD entry, can		 * lead to them sequentially updating the PMD with the		 * same value. Following the break-before-make		 * (pmd_clear() followed by tlb_flush()) process can		 * hinder forward progress due to refaults generated		 * on missing translations.		 *		 * Skip updating the page table if the entry is		 * unchanged.		 */		if (pmd_val(old_pmd) == pmd_val(*new_pmd))			return 0;		/*		 * Mapping in huge pages should only happen through a		 * fault.  If a page is merged into a transparent huge		 * page, the individual subpages of that huge page		 * should be unmapped through MMU notifiers before we		 * get here.		 *		 * Merging of CompoundPages is not supported; they		 * should become splitting first, unmapped, merged,		 * and mapped back in on-demand.		 */		VM_BUG_ON(pmd_pfn(old_pmd) != pmd_pfn(*new_pmd));		pmd_clear(pmd);		kvm_tlb_flush_vmid_ipa(kvm, addr);	}
if (pmd_val(old_pmd) == pmd_val(*new_pmd))			return 0;
pmd_val(old_pmd) == pmd_val(*new_pmd)
pmd_val(old_pmd)
pmd_val
pmd_val
old_pmd
old_pmd
pmd_val(*new_pmd)
pmd_val
pmd_val
*new_pmd
new_pmd
new_pmd
return 0;
0
VM_BUG_ON(pmd_pfn(old_pmd) != pmd_pfn(*new_pmd));
VM_BUG_ON(pmd_pfn(old_pmd) != pmd_pfn(*new_pmd))
VM_BUG_ON
VM_BUG_ON
pmd_pfn(old_pmd) != pmd_pfn(*new_pmd)
pmd_pfn(old_pmd)
pmd_pfn
pmd_pfn
old_pmd
old_pmd
pmd_pfn(*new_pmd)
pmd_pfn
pmd_pfn
*new_pmd
new_pmd
new_pmd
pmd_clear(pmd);
pmd_clear(pmd)
pmd_clear
pmd_clear
pmd
pmd
kvm_tlb_flush_vmid_ipa(kvm, addr);
kvm_tlb_flush_vmid_ipa(kvm, addr)
kvm_tlb_flush_vmid_ipa
kvm_tlb_flush_vmid_ipa
kvm
kvm
addr
addr
{		get_page(virt_to_page(pmd));	}
get_page(virt_to_page(pmd));
get_page(virt_to_page(pmd))
get_page
get_page
virt_to_page(pmd)
virt_to_page
virt_to_page
pmd
pmd
kvm_set_pmd(pmd, *new_pmd);
kvm_set_pmd(pmd, *new_pmd)
kvm_set_pmd
kvm_set_pmd
pmd
pmd
*new_pmd
new_pmd
new_pmd
return 0;
0
-----joern-----
(24,34,0)
(11,46,0)
(44,3,0)
(39,50,0)
(55,35,0)
(33,50,0)
(46,11,0)
(18,10,0)
(23,9,0)
(6,44,0)
(45,14,0)
(53,11,0)
(2,1,0)
(35,44,0)
(13,32,0)
(30,3,0)
(10,8,0)
(2,21,0)
(32,3,0)
(46,14,0)
(51,40,0)
(38,54,0)
(42,30,0)
(19,42,0)
(27,8,0)
(13,30,0)
(35,47,0)
(34,41,0)
(34,25,0)
(46,48,0)
(11,53,0)
(7,26,0)
(52,3,0)
(31,34,0)
(44,30,0)
(44,35,0)
(14,8,0)
(53,47,0)
(8,10,0)
(9,30,0)
(26,54,0)
(49,34,0)
(17,13,0)
(4,33,0)
(0,30,0)
(26,25,0)
(33,47,0)
(34,29,0)
(9,54,0)
(10,30,0)
(8,3,0)
(34,30,0)
(14,46,0)
(22,3,0)
(16,36,0)
(12,21,0)
(37,8,0)
(32,13,0)
(39,37,0)
(15,3,0)
(20,37,0)
(48,46,0)
(43,26,0)
(42,51,0)
(28,1,0)
(50,39,0)
(21,3,0)
(37,39,0)
(5,53,0)
(48,54,0)
(30,34,0)
(50,33,0)
(26,41,0)
(36,8,0)
(51,42,0)
(32,13,1)
(35,55,1)
(33,4,1)
(44,30,1)
(55,6,1)
(18,27,1)
(7,43,1)
(42,30,1)
(9,30,1)
(13,17,1)
(10,18,1)
(39,50,1)
(34,41,1)
(2,21,1)
(14,8,1)
(5,14,1)
(46,14,1)
(42,19,1)
(33,47,1)
(48,46,1)
(46,11,1)
(43,9,1)
(26,7,1)
(36,8,1)
(4,37,1)
(34,31,1)
(12,44,1)
(44,35,1)
(19,36,1)
(50,33,1)
(34,29,1)
(34,25,1)
(16,8,1)
(1,28,1)
(28,39,1)
(10,30,1)
(20,36,1)
(39,37,1)
(26,25,1)
(27,32,1)
(6,51,1)
(53,5,1)
(23,48,1)
(26,41,1)
(37,8,1)
(53,47,1)
(2,1,1)
(31,49,1)
(9,23,1)
(6,26,1)
(8,10,1)
(24,0,1)
(35,47,1)
(49,24,1)
(11,53,1)
(45,39,1)
(13,30,1)
(37,20,1)
(17,30,1)
(14,45,1)
(21,12,1)
(36,16,1)
(51,42,1)
(30,34,1)
(21,39,2)
(23,39,2)
(12,36,2)
(48,39,2)
(12,39,2)
(33,47,2)
(35,39,2)
(34,25,2)
(32,13,2)
(9,39,2)
(9,30,2)
(26,25,2)
(11,39,2)
(53,47,2)
(33,36,2)
(48,46,2)
(2,21,2)
(7,39,2)
(44,30,2)
(46,14,2)
(13,30,2)
(28,39,2)
(44,35,2)
(39,36,2)
(4,36,2)
(5,39,2)
(6,36,2)
(44,36,2)
(55,36,2)
(34,41,2)
(8,10,2)
(6,39,2)
(55,39,2)
(39,50,2)
(45,39,2)
(37,36,2)
(39,37,2)
(46,39,2)
(26,39,2)
(35,36,2)
(26,41,2)
(36,8,2)
(2,1,2)
(21,36,2)
(51,36,2)
(50,33,2)
(50,36,2)
(19,36,2)
(14,8,2)
(46,11,2)
(42,30,2)
(42,36,2)
(30,34,2)
(51,42,2)
(20,36,2)
(14,39,2)
(11,53,2)
(53,39,2)
(34,29,2)
(35,47,2)
(37,8,2)
(1,39,2)
(10,30,2)
(44,39,2)
(43,39,2)
-----------------------------------
(0,pmd)
(1,return 0;)
(2,RET)
(3,)
(4,new_pmd)
(5,new_pmd)
(6,pmd)
(7,addr)
(8,old_pmd = *pmd)
(9,pmd_clear(pmd)
(10,*pmd)
(11,pmd_pfn(*new_pmd)
(12,0)
(13,!pmd)
(14,pmd_pfn(old_pmd)
(15,if (pmd_present(old_pmd)
(16,old_pmd)
(17,pmd)
(18,pmd)
(19,pmd)
(20,old_pmd)
(21,return 0;)
(22,pmd)
(23,pmd)
(24,kvm)
(25,struct kvm *kvm)
(26,kvm_tlb_flush_vmid_ipa(kvm, addr)
(27,old_pmd)
(28,0)
(29,struct kvm_mmu_memory_cache\n\\n\\t\\t\\t       *cache)
(30,pmd = stage2_get_pmd(kvm, cache, addr)
(31,addr)
(32,VM_BUG_ON(!pmd)
(33,*new_pmd)
(34,stage2_get_pmd(kvm, cache, addr)
(35,*new_pmd)
(36,pmd_present(old_pmd)
(37,pmd_val(old_pmd)
(38,if (pmd_val(old_pmd)
(39,pmd_val(old_pmd)
(40,)
(41,phys_addr_t addr)
(42,virt_to_page(pmd)
(43,kvm)
(44,kvm_set_pmd(pmd, *new_pmd)
(45,old_pmd)
(46,pmd_pfn(old_pmd)
(47,const pmd_t *new_pmd)
(48,VM_BUG_ON(pmd_pfn(old_pmd)
(49,cache)
(50,pmd_val(*new_pmd)
(51,get_page(virt_to_page(pmd)
(52,old_pmd)
(53,*new_pmd)
(54,)
(55,new_pmd)
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^